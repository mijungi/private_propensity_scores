{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of $\\epsilon$ on Average Treatment Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12d8d7350>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from scipy.optimize import minimize, minimize_scalar\n",
    "from scipy.special import erf, gamma\n",
    "import torch\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from misc.agm import calibrateAnalyticGaussianMechanism\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pdb\n",
    "\n",
    "# set random seed\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no. experiments, no. samples, dim, X\n",
    "ne = 500\n",
    "ns = 2000\n",
    "dim = 50\n",
    "\n",
    "# true treatment effect tau\n",
    "tau = 0.1\n",
    "\n",
    "# draw ne separate ns samples\n",
    "X_std = 3\n",
    "X_dist = torch.distributions.normal.Normal(\n",
    "    torch.tensor([0.0], dtype=torch.float64), \n",
    "    torch.tensor([X_std], dtype=torch.float64)\n",
    ")\n",
    "X  = [X_dist.sample((ns, dim)).squeeze() for i in range(ne)]\n",
    "\n",
    "# restrict X to ||x||_2 \\leq 1 to fit assumption for each experiment\n",
    "X = torch.stack([X[i] / X[i].norm(dim=1).max() for i in range(ne)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no. of points used to fit log reg\n",
    "nf = 1000\n",
    "\n",
    "# privacy parameters\n",
    "# epses = [0, 0.01, 0.03, 0.05, 0.1, 0.2, 0.4, 0.8, 0.99]\n",
    "epses = [0, 0.03, 0.05, 0.1, 0.2, 0.4, 0.8, 0.99]\n",
    "# epses = [0, 0.05, 0.1, 0.2, 0.4, 0.8, 0.99]\n",
    "delta = torch.tensor(1e-6)\n",
    "\n",
    "# regularisation coefficient\n",
    "reg_co = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectors for generating Y, differentiate between Y_0 and Y_1 with true treatment effect tau\n",
    "# Y = beta^T X + 0.1 Z\n",
    "# Y_1 = Y + tau, Y_0 = Y\n",
    "beta_std = 1\n",
    "beta_dist = torch.distributions.normal.Normal(\n",
    "    torch.tensor([0], dtype=torch.float64), \n",
    "    torch.tensor([beta_std], dtype=torch.float64)\n",
    ")\n",
    "beta = beta_dist.sample((dim, 1)).reshape(dim, 1)\n",
    "\n",
    "# generate Y\n",
    "Y_std = 0.1\n",
    "Y = torch.einsum('kl,ijk->ij', beta, X) + Y_std * torch.randn(ne, ns, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectors for generating T\n",
    "# T = exp(-T_w^T X + b)\n",
    "T_std = 1\n",
    "T_dist = torch.distributions.normal.Normal(\n",
    "    torch.tensor([0.0], dtype=torch.float64), \n",
    "    torch.tensor([T_std], dtype=torch.float64)\n",
    ")\n",
    "T_w = T_dist.sample((dim, 1)).reshape(dim, 1)\n",
    "T_b = 0\n",
    "\n",
    "# generate T\n",
    "prob_vec = torch.sigmoid(torch.einsum('kl,ijk->ij', T_w, X) + T_b)\n",
    "T = torch.bernoulli(prob_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Log_Reg(torch.nn.Module):\n",
    "    '''\n",
    "    Logistic Regression\n",
    "    '''\n",
    "    def __init__(self, D_in, D_out):\n",
    "        super(Log_Reg, self).__init__()\n",
    "        self.linear = torch.nn.Linear(D_in, D_out, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = torch.sigmoid(self.linear(x))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPW_PPS_Out(X, T, prob_vec, Y, tau, epses, delta, reg_co, nf):\n",
    "    '''\n",
    "    average treatment effect with inverse propensity weighting using private propensity scores\n",
    "    '''\n",
    "    # get # experiments, # samples, # dimensions\n",
    "    ne, ns, dim = X.shape\n",
    "\n",
    "    ################\n",
    "    # process data #\n",
    "    ################\n",
    "\n",
    "    # get Y0 and Y1\n",
    "    Y0 = Y * (1 - T)\n",
    "    Y1 = (Y + tau) * T\n",
    "    \n",
    "    # split data\n",
    "    # get splits\n",
    "    fit_split = nf\n",
    "    est_split = ns - nf\n",
    "\n",
    "    # permute indices\n",
    "    perm = torch.stack(\n",
    "        [torch.randperm(ns) for i in range(ne)]\n",
    "    )\n",
    "\n",
    "    # create splits\n",
    "    s0 = perm[:, :fit_split]\n",
    "    s1 = perm[:, fit_split:]\n",
    "\n",
    "    # create auxiliary indices\n",
    "    idx = torch.arange(ne)[:, None]\n",
    "\n",
    "    # split X into fit, estimate splits\n",
    "    X_s0 = X[idx, s0]\n",
    "    X_s1 = X[idx, s1]\n",
    "\n",
    "    # expand dim of T to allow multiplication with X\n",
    "    T_ex_dim = T.reshape(ne, ns, 1)\n",
    "\n",
    "    # split X0 and X1 into untreated, treated\n",
    "    X0_s1 = (X * (1 - T_ex_dim))[idx, s1]\n",
    "    X1_s1 = (X * T_ex_dim)[idx, s1]\n",
    "    \n",
    "    # compute appropriate X cross products\n",
    "    cp_X0_s1 = torch.einsum('ijk,ilk->ijl', X0_s1, X0_s1)\n",
    "    cp_X1_s1 = torch.einsum('ijk,ilk->ijl', X1_s1, X1_s1)\n",
    "    cp_X0_X1_s1 = torch.einsum('ijk,ilk->ijl', X0_s1, X1_s1)\n",
    "    \n",
    "    X_sl_sq_L2 = X_s1.pow(2).sum(2, keepdim=True)\n",
    "\n",
    "    cp_X_s1_sign = (X_sl_sq_L2 + X_sl_sq_L2.transpose(2, 1) \n",
    "                    + 2 * cp_X1_s1 + 2 * cp_X0_s1 \n",
    "                    - 2 * cp_X0_X1_s1 - 2 * cp_X0_X1_s1.transpose(2, 1))\n",
    "        \n",
    "    for expm in range(ne):\n",
    "        cp_X_s1_sign[expm, range(est_split), range(est_split)] = 4 * cp_X_s1_sign[expm, range(est_split), range(est_split)]\n",
    "\n",
    "    # split T into fit, estimate splits\n",
    "    T_s0 = T[idx, s0]\n",
    "    T_s1 = T[idx, s1]\n",
    "\n",
    "    # split Y0 and Y1 into fit, estimate splits\n",
    "    Y0_s0 = Y0[idx, s0]\n",
    "    Y1_s0 = Y1[idx, s0]\n",
    "\n",
    "    Y0_s1 = Y0[idx, s1]\n",
    "    Y1_s1 = Y1[idx, s1]\n",
    "    \n",
    "    # compute Y cross products\n",
    "    Y_s1 = Y0_s1 + Y1_s1\n",
    "    \n",
    "    cp_Y0_s1 = torch.einsum('ij,ik->ijk', Y0_s1, Y0_s1)\n",
    "    cp_Y1_s1 = torch.einsum('ij,ik->ijk', Y1_s1, Y1_s1)\n",
    "    cp_Y0_Y1_s1 = torch.einsum('ij,ik->ijk', Y0_s1, Y1_s1)\n",
    "    \n",
    "    cp_Y = cp_Y1_s1 + cp_Y0_s1 - cp_Y0_Y1_s1\n",
    "    \n",
    "    ##############\n",
    "    # fit models #\n",
    "    ##############\n",
    "    \n",
    "    models = []\n",
    "    \n",
    "    for expm in range(ne):\n",
    "        X = Variable(X_s0[expm], requires_grad=True)\n",
    "        T = Variable(T_s0[expm][:, None])\n",
    "        model = Log_Reg(dim, 1).double()\n",
    "        opt = torch.optim.LBFGS(model.parameters(), max_iter=100)\n",
    "\n",
    "        # define first-order oracle for lbfgs\n",
    "        def closure():\n",
    "            if torch.is_grad_enabled():\n",
    "                opt.zero_grad()\n",
    "            outputs = model(X)\n",
    "            for weights in model.parameters():\n",
    "                loss = torch.nn.functional.binary_cross_entropy_with_logits(outputs, T) + 0.5 * reg_co * weights.norm(2).pow(2)\n",
    "            if loss.requires_grad:\n",
    "                loss.backward()\n",
    "            return loss\n",
    "\n",
    "        opt.step(closure)\n",
    "\n",
    "        models.append(model)\n",
    "        \n",
    "        \n",
    "    ###########################\n",
    "    # optimise epsilon splits #\n",
    "    ###########################\n",
    "        \n",
    "    # get estimated taus\n",
    "    pi_hats = torch.stack(\n",
    "        [models[i](X_s1[i]).squeeze() for i in range(ne)]\n",
    "    )\n",
    "\n",
    "    te_hats = torch.mean(\n",
    "        Y1_s1 / pi_hats - Y0_s1 / (1 - pi_hats),\n",
    "        1,\n",
    "    )\n",
    "                \n",
    "    def _choose_eps(x, eps, expm, models, T_s1, Y_s1, te_hats):    \n",
    "        # define sensitivity for log reg\n",
    "        s_w = 2.0 / (fit_split * reg_co)\n",
    "\n",
    "        # define sigma for log reg\n",
    "        sigma = np.sqrt(\n",
    "            2 * np.log(1.25 / delta) + 1e-10\n",
    "        ) * (s_w / x)\n",
    "        sigma_sq = sigma ** 2\n",
    "\n",
    "        # sigma^2 multiplied by cross products of X_s1\n",
    "        exp_sigma_sq_cp = np.exp(sigma_sq / 2 * cp_X_s1_sign[expm])    \n",
    "\n",
    "        # compute E_z[\\hat{tau}_n], E_z[\\hat{tau}_n^2] \n",
    "        weights = models[expm].linear.weight\n",
    "        # exp(X_s1 w^T)\n",
    "        T = T_s1[expm]\n",
    "        exp_w_T_X = ((1 - T) * torch.exp(torch.einsum('jk, lk -> j', X0_s1[expm], weights))\n",
    "                     + T * torch.exp(torch.einsum('jk, lk -> j', X1_s1[expm], -weights)))\n",
    "        # E_z[\\hat{\\tau}_n]\n",
    "        exp_sigma_sq = np.exp(sigma_sq / 2 * X_sl_sq_L2[expm].squeeze())\n",
    "        tau_T_1 = Y_s1[expm] * T * (1 + exp_w_T_X * exp_sigma_sq) \n",
    "        tau_T_0 = Y_s1[expm] * (1 - T) * (1 + exp_w_T_X * exp_sigma_sq)\n",
    "        E_tau_hat_n = (tau_T_1 - tau_T_0).mean()\n",
    "        \n",
    "        # E_z[\\hat{\\tau}_n^2]\n",
    "        # cross addition\n",
    "        ca_exp_w_T_X = exp_w_T_X + exp_w_T_X.T\n",
    "        # cross product\n",
    "        cp_exp_w_T_X = exp_w_T_X * exp_w_T_X.T\n",
    "                                 \n",
    "        mat_E_tau_hat_n_sq = cp_Y[expm] * (1 + ca_exp_w_T_X + cp_exp_w_T_X * exp_sigma_sq_cp)\n",
    "        E_tau_hat_n_sq = (2 * mat_E_tau_hat_n_sq - torch.diag(torch.diag(mat_E_tau_hat_n_sq))).mean()\n",
    "        \n",
    "        # variance of second stage output perturbation \n",
    "        E_S_tau_hat_n = 2 * (tau_T_1.abs() + tau_T_0.abs()).max() / est_split\n",
    "        sigma_tau = np.sqrt(\n",
    "            2 * np.log(1.25 / delta) + 1e-10\n",
    "        ) * (E_S_tau_hat_n / (eps - x))\n",
    "        sigma_tau_sq = sigma_tau ** 2\n",
    "        \n",
    "        return (- te_hats[expm] * E_tau_hat_n + E_tau_hat_n_sq + sigma_tau_sq).detach().numpy()\n",
    "\n",
    "\n",
    "    # initialise eps dictionaries\n",
    "    eps_split = {}\n",
    "    \n",
    "    for eps in epses[1:]:\n",
    "        eps_split[eps] = []\n",
    "        for expm in range(ne):\n",
    "            out = minimize_scalar(\n",
    "                partial(_choose_eps, eps=eps, expm=expm, models=models, T_s1=T_s1, Y_s1=Y_s1, te_hats=te_hats),\n",
    "                bounds=(0, eps), \n",
    "                method='bounded'\n",
    "            )\n",
    "            eps_split[eps].append(out.x)\n",
    "            \n",
    "\n",
    "    #############################\n",
    "    # estimate treatment effect #\n",
    "    #############################\n",
    "\n",
    "    # initialise pi_hat dictionaries\n",
    "    pi_hats = {}\n",
    "    \n",
    "    # initialise e dictionary\n",
    "    e = {}\n",
    "    \n",
    "    # intialise sigma dictionary\n",
    "    sig_d = {}\n",
    "\n",
    "    # get estimated propensity scores\n",
    "    pi_hats[0] = torch.stack(\n",
    "        [models[i](X_s1[i]).squeeze() for i in range(ne)]\n",
    "    )\n",
    "\n",
    "    # perturb model and get relevant quantities\n",
    "    for eps in epses[1:]:\n",
    "        # define sensitivity for log reg\n",
    "        s_w = 2.0 / (fit_split * reg_co)\n",
    "\n",
    "        # create temp models\n",
    "        models_ = copy.deepcopy(models)\n",
    "\n",
    "        # initialise lists for specific eps\n",
    "        pi_hats[eps] = []\n",
    "        e[eps] = []\n",
    "        sig_d[eps] = []\n",
    "        \n",
    "        # perturb weights with z_vecs\n",
    "        for i in range(ne):\n",
    "            # define sigma for log reg\n",
    "            sigma = np.sqrt(\n",
    "                2 * np.log(1.25 / delta) + 1e-10\n",
    "            ) * (s_w / eps_split[eps][i])\n",
    "            \n",
    "            # define z distribution for log reg\n",
    "            z_dist = torch.distributions.normal.Normal(\n",
    "                torch.tensor(0.0, dtype=torch.float64),\n",
    "                torch.tensor(sigma, dtype=torch.float64),\n",
    "            )\n",
    "\n",
    "            model_temp = models_[i]\n",
    "            model_temp.linear.weight.data.add_(\n",
    "                z_dist.sample((1, dim))\n",
    "            )\n",
    "            pi_hats[eps].append(\n",
    "                model_temp(X_s1[i]).squeeze()\n",
    "            )\n",
    "                                 \n",
    "            # max of abs of Y1_s1 / propensity score for each experiment\n",
    "            max_abs_Y1_s1_div_ps = torch.max(\n",
    "                torch.abs(Y1_s1[i]) / (est_split * pi_hats[eps][-1])\n",
    "            )\n",
    "        \n",
    "            # max of abs of Y0_s1 / (1 - propensity score) for each experiment\n",
    "            max_abs_Y0_s1_div_1_m_ps = torch.max(\n",
    "                torch.abs(Y0_s1[i]) / (est_split * (1 - pi_hats[eps][-1]))\n",
    "            )\n",
    "        \n",
    "            # hstack max_abs_Y_s1_div_ps and max_abs_Y_s1_div_1_m_ps\n",
    "            max_abs_all = torch.stack(\n",
    "                (max_abs_Y1_s1_div_ps, max_abs_Y0_s1_div_1_m_ps)\n",
    "            )\n",
    "        \n",
    "            # replace inf/nan with 1e20 for stability\n",
    "            max_abs_all[torch.isfinite(max_abs_all) == 0] = 1e20\n",
    "\n",
    "            # define sensitivity for estimation\n",
    "            s_e = 2 * torch.max(max_abs_all)\n",
    "\n",
    "            # define sigma for estimation\n",
    "            sigma_e = np.sqrt(\n",
    "                2 * np.log(1.25 / delta) + 1e-10\n",
    "            ) * (s_e / (eps - eps_split[eps][i]))\n",
    "            sig_d[eps].append(sigma_e)\n",
    "\n",
    "            # define e distribution for estimation\n",
    "            e_dist = torch.distributions.normal.Normal(\n",
    "                torch.tensor([0.0], dtype=torch.float64),\n",
    "                sigma_e\n",
    "            )\n",
    "\n",
    "            # draw e for estimation\n",
    "            e[eps].append(e_dist.sample())\n",
    "\n",
    "        # stack lists\n",
    "        pi_hats[eps] = torch.stack(pi_hats[eps])\n",
    "        e[eps] = torch.stack(e[eps]).squeeze()\n",
    "        sig_d[eps] = torch.stack(sig_d[eps]).detach().numpy()\n",
    "           \n",
    "    # get treatment effects\n",
    "    # true\n",
    "    te = {}\n",
    "    # empirical means and std of means of ERM + private ERM\n",
    "    te_hats = {'means': [], 'stds': []}\n",
    "    # means and std of means of privatised te_hats\n",
    "    te_hats_p = {'means': [], 'stds': []}\n",
    "\n",
    "    # estimate true treatment effect\n",
    "    te_ = torch.mean(\n",
    "        Y1_s1 / prob_vec[idx, s1] - Y0_s1 / (1 - prob_vec[idx, s1]),\n",
    "        1,\n",
    "    )\n",
    "    te['mean'] = te_.mean().detach().numpy()\n",
    "    te['std'] = te_.std().detach().numpy()\n",
    "                \n",
    "    for key in pi_hats.keys():\n",
    "        # empirical estimate for noiseless case\n",
    "        # reduce_mean from (ne, est_split) tensor to (ne , 1) matrix\n",
    "        te_hats_ = torch.mean(\n",
    "            Y1_s1 / pi_hats[key] - Y0_s1 / (1 - pi_hats[key]),\n",
    "            1,\n",
    "        )\n",
    "        te_hats['means'].append(\n",
    "            te_hats_.detach().numpy()\n",
    "        )\n",
    "        te_hats['stds'].append(\n",
    "            te_hats_.std().detach().numpy()\n",
    "        )\n",
    "        try:\n",
    "            te_hats_p_ = te_hats_ + e[key]\n",
    "            te_hats_p['means'].append(\n",
    "                te_hats_p_.detach().numpy()\n",
    "            )\n",
    "            te_hats_p['stds'].append(\n",
    "                te_hats_p_.std().detach().numpy()\n",
    "            )\n",
    "        except KeyError:\n",
    "            # fill first row for later\n",
    "            te_hats_p['means'].append(\n",
    "                te_hats_.detach().numpy()\n",
    "            )\n",
    "            te_hats_p['stds'].append(\n",
    "                te_hats_.std().detach().numpy()\n",
    "            )\n",
    "        \n",
    "    te_hats['means'] = np.array(te_hats['means'])\n",
    "    te_hats['stds'] = np.array(te_hats['stds'])\n",
    "    te_hats_p['means'] = np.array(te_hats_p['means'])\n",
    "    te_hats_p['stds'] = np.array(te_hats_p['stds'])\n",
    "\n",
    "    return te, te_hats, te_hats_p, sig_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPW_PPS_Obj(X, T, prob_vec, Y, tau, epses, delta, reg_co, nf):\n",
    "    '''\n",
    "    average treatment effect with inverse propensity weighting using private propensity scores\n",
    "    '''\n",
    "    # get # experiments, # samples, # dimensions\n",
    "    ne, ns, dim = X.shape\n",
    "\n",
    "    # objective perturbation constants\n",
    "    L = 1 # see from derivation, also http://proceedings.mlr.press/v32/jain14.pdf\n",
    "    R2 = 1 # as norm is bounded by 1\n",
    "    c = 0.25\n",
    "    \n",
    "    ################\n",
    "    # process data #\n",
    "    ################\n",
    "\n",
    "    # get Y0 and Y1\n",
    "    Y0 = Y * (1 - T)\n",
    "    Y1 = (Y + tau) * T\n",
    "    \n",
    "    # split data\n",
    "    # get splits\n",
    "    fit_split = nf\n",
    "    est_split = ns - nf\n",
    "\n",
    "    # permute indices\n",
    "    perm = torch.stack(\n",
    "        [torch.randperm(ns) for i in range(ne)]\n",
    "    )\n",
    "\n",
    "    # create splits\n",
    "    s0 = perm[:, :fit_split]\n",
    "    s1 = perm[:, fit_split:]\n",
    "\n",
    "    # create auxiliary indices\n",
    "    idx = torch.arange(ne)[:, None]\n",
    "\n",
    "    # split X into fit, estimate splits\n",
    "    X_s0 = X[idx, s0]\n",
    "    X_s1 = X[idx, s1]\n",
    "\n",
    "    # expand dim of T to allow multiplication with X\n",
    "    T_ex_dim = T.reshape(ne, ns, 1)\n",
    "\n",
    "    # split X0 and X1 into untreated, treated\n",
    "    X0_s1 = (X * (1 - T_ex_dim))[idx, s1]\n",
    "    X1_s1 = (X * T_ex_dim)[idx, s1]\n",
    "    \n",
    "    # compute appropriate X cross products\n",
    "    cp_X0_s1 = torch.einsum('ijk,ilk->ijl', X0_s1, X0_s1)\n",
    "    cp_X1_s1 = torch.einsum('ijk,ilk->ijl', X1_s1, X1_s1)\n",
    "    cp_X0_X1_s1 = torch.einsum('ijk,ilk->ijl', X0_s1, X1_s1)\n",
    "    \n",
    "    X_sl_sq_L2 = X_s1.pow(2).sum(2, keepdim=True)\n",
    "\n",
    "    cp_X_s1_sign = (X_sl_sq_L2 + X_sl_sq_L2.transpose(2, 1) \n",
    "                    + 2 * cp_X1_s1 + 2 * cp_X0_s1 \n",
    "                    - 2 * cp_X0_X1_s1 - 2 * cp_X0_X1_s1.transpose(2, 1))\n",
    "            \n",
    "\n",
    "    # split T into fit, estimate splits\n",
    "    T_s0 = T[idx, s0]\n",
    "    T_s1 = T[idx, s1]\n",
    "\n",
    "    # split Y0 and Y1 into fit, estimate splits\n",
    "    Y0_s0 = Y0[idx, s0]\n",
    "    Y1_s0 = Y1[idx, s0]\n",
    "\n",
    "    Y0_s1 = Y0[idx, s1]\n",
    "    Y1_s1 = Y1[idx, s1]\n",
    "    \n",
    "    # compute Y cross products\n",
    "    Y_s1 = (Y0_s1 + Y1_s1).unsqueeze(2)\n",
    "    cp_Y = Y_s1 * Y_s1.transpose(2, 1)\n",
    "    \n",
    "    # Gamma(dim + 0.5) / Gamma(dim)\n",
    "    gamma_constant = (2. ** 0.5) * gamma(dim + 0.5) / gamma(dim)\n",
    "    \n",
    "    #######################\n",
    "    # fit noiseless model #\n",
    "    #######################\n",
    "\n",
    "    models = {}\n",
    "    \n",
    "    models[0] = []\n",
    "    \n",
    "    for expm in range(ne):\n",
    "        X = Variable(X_s0[expm], requires_grad=True)\n",
    "        T = Variable(T_s0[expm][:, None].double())\n",
    "        model = Log_Reg(dim, 1).double()\n",
    "        opt = torch.optim.LBFGS(model.parameters(), max_iter=100)\n",
    "    \n",
    "        # define first-order oracle for lbfgs\n",
    "        def closure():\n",
    "            if torch.is_grad_enabled():\n",
    "                opt.zero_grad()\n",
    "            outputs = model(X)\n",
    "            for weights in model.parameters():\n",
    "                reg_noise = 0.5 * reg_co * weights.norm(2).pow(2)\n",
    "            loss = torch.nn.functional.binary_cross_entropy_with_logits(outputs, T) + reg_noise\n",
    "            if loss.requires_grad:\n",
    "                loss.backward()\n",
    "            return loss\n",
    "\n",
    "        opt.step(closure)\n",
    "        \n",
    "        models[0].append(model)\n",
    "  \n",
    "    ###########################\n",
    "    # optimise epsilon splits #\n",
    "    ###########################\n",
    "  \n",
    "    # initialise pi_hats dictionary\n",
    "    pi_hats = {}\n",
    "\n",
    "    # get estimated taus\n",
    "    pi_hats[0] = torch.stack(\n",
    "        [models[0][i](X_s1[i]).squeeze() for i in range(ne)]\n",
    "    )\n",
    "\n",
    "    te_hats = torch.mean(\n",
    "        Y1_s1 / pi_hats[0] - Y0_s1 / (1 - pi_hats[0]),\n",
    "        1,\n",
    "    )\n",
    "  \n",
    "    def _choose_eps(x, eps, expm, models, T_s1, Y_s1, te_hats):    \n",
    "        # E_z[||z||_2]\n",
    "        eig_val, _ = torch.symeig(torch.matmul(X_s0[expm].T, X_s0[expm]))\n",
    "        Delta = 0.25 * 2 * torch.max(eig_val) / x\n",
    "        eta = 0.5 * reg_co * nf\n",
    "\n",
    "        sigma = torch.sqrt((8 * (torch.log(2. / delta) + 4 * x)) / (x ** 2))\n",
    "\n",
    "        # compute E_z[\\hat{tau}_n], E_z[\\hat{tau}_n^2] \n",
    "        weights = models[expm].linear.weight\n",
    "        E_Z = sigma * gamma_constant\n",
    "        E_weight_bias_L2 = 2 * E_Z / (Delta + eta) + torch.sqrt(Delta / (Delta + eta)) * torch.norm(weights)\n",
    "        \n",
    "        # sigma multiplied by cross products of X_s1\n",
    "        exp_sigma_cp = torch.exp(E_weight_bias_L2 * cp_X_s1_sign[expm])    \n",
    "        \n",
    "        # exp(X_s1 w^T)\n",
    "        T = T_s1[expm].reshape(-1, 1)\n",
    "        exp_w_T_X = ((1 - T) * torch.exp(torch.einsum('jk, lk -> jl', X0_s1[expm], weights))\n",
    "                     + T * torch.exp(torch.einsum('jk, lk -> jl', X1_s1[expm], -weights)))\n",
    "        # E_z[\\hat{\\tau}_n]\n",
    "        exp_sigma = torch.exp(E_weight_bias_L2 * torch.sqrt(X_sl_sq_L2[expm])).reshape(-1, 1)\n",
    "        tau_T_1 = Y_s1[expm] * T * (1 + exp_w_T_X * exp_sigma) \n",
    "        tau_T_0 = Y_s1[expm] * (1 - T) * (1 + exp_w_T_X * exp_sigma)\n",
    "        E_tau_hat_n = (tau_T_1 - tau_T_0).mean()\n",
    "        \n",
    "        # cross addition\n",
    "        ca_exp_w_T_X = exp_w_T_X + exp_w_T_X.T\n",
    "        # cross product\n",
    "        cp_exp_w_T_X = exp_w_T_X * exp_w_T_X.T\n",
    "        # E_z[\\hat{\\tau}_n^2]\n",
    "        mat_E_tau_hat_n_sq = cp_Y[expm] * (1 + ca_exp_w_T_X + cp_exp_w_T_X * exp_sigma_cp)\n",
    "        E_tau_hat_n_sq = (2 * mat_E_tau_hat_n_sq - torch.diag(torch.diag(mat_E_tau_hat_n_sq))).mean()\n",
    "        \n",
    "        # variance of second stage output perturbation \n",
    "        E_S_tau_hat_n = 2 * (tau_T_1.abs() + tau_T_0.abs()).max() / (ns - nf)\n",
    "        sigma_tau = np.sqrt(\n",
    "            2 * np.log(1.25 / delta) + 1e-10\n",
    "        ) * (E_S_tau_hat_n.detach().numpy() / (eps - x))\n",
    "        sigma_tau_sq = sigma_tau ** 2\n",
    "        \n",
    "        return (- te_hats[expm] * E_tau_hat_n + E_tau_hat_n_sq + sigma_tau_sq).detach().numpy()\n",
    "\n",
    "    # initialise eps dictionaries\n",
    "    eps_split = {}\n",
    "    \n",
    "    for eps in epses[1:]:\n",
    "        eps_split[eps] = []\n",
    "        for expm in range(ne):\n",
    "#             out = minimize(\n",
    "#                 partial(_choose_eps, eps=eps, expm=expm, models=models[0], T_s1=T_s1, Y_s1=Y_s1, te_hats=te_hats),\n",
    "#                 [0.5 * eps],\n",
    "#                 bounds=((0, eps), ), \n",
    "#                 method='Powell',\n",
    "#             )\n",
    "            out = minimize_scalar(\n",
    "                partial(_choose_eps, eps=eps, expm=expm, models=models[0], T_s1=T_s1, Y_s1=Y_s1, te_hats=te_hats),\n",
    "                bounds=(0, eps), \n",
    "                method='bounded'\n",
    "            )\n",
    "            \n",
    "            eps_split[eps].append(out.x)\n",
    "#             print(out)\n",
    "    \n",
    "#     pdb.set_trace()\n",
    "    \n",
    "    ####################\n",
    "    # fit noisy models #\n",
    "    ####################\n",
    "    \n",
    "    z_dist = torch.distributions.normal.Normal(\n",
    "                torch.tensor(0.0, dtype=torch.double),\n",
    "                torch.tensor(1.0, dtype=torch.double),\n",
    "                )\n",
    "    \n",
    "    for eps in epses[1:]:\n",
    "        models[eps] = []\n",
    "        for expm in range(ne):\n",
    "            # follows from http://proceedings.mlr.press/v23/kifer12/kifer12.pdf,\n",
    "            # hessian is bounded by 0.25 * max_eig_val, see https://www.cs.ubc.ca/~schmidtm/Courses/540-W18/L4.pdf\n",
    "            eig_val, _ = torch.symeig(torch.matmul(X_s0[expm].T, X_s0[expm]))\n",
    "            Delta = c * 2 * torch.max(eig_val) / eps_split[eps][expm]\n",
    "            X = Variable(X_s0[expm], requires_grad=True)\n",
    "            T = Variable(T_s0[expm][:, None].double())\n",
    "            model = Log_Reg(dim, 1).double()\n",
    "            opt = torch.optim.LBFGS(model.parameters(), max_iter=100)\n",
    "            if eps > 0: \n",
    "                reg_co_ = reg_co\n",
    "                # zeta =< 1 for logistic loss\n",
    "                b = torch.sqrt((8 * (torch.log(2. / delta) + 4 * eps_split[eps][expm])) / (eps_split[eps][expm] ** 2)) * z_dist.sample((dim, 1))\n",
    "            \n",
    "            # define first-order oracle for lbfgs\n",
    "            def closure():\n",
    "                if torch.is_grad_enabled():\n",
    "                    opt.zero_grad()\n",
    "                outputs = model(X)\n",
    "                if eps > 0:\n",
    "                    for weights in model.parameters():\n",
    "                        reg_noise = (1 / nf) * torch.matmul(weights, b) + 0.5 * (reg_co_ + Delta / nf) * weights.pow(2).sum(1) \n",
    "                else:\n",
    "                    for weights in model.parameters():\n",
    "                        reg_noise = 0.5 * reg_co * weights.norm(2).pow(2)\n",
    "                loss = torch.nn.functional.binary_cross_entropy_with_logits(outputs, T) + reg_noise\n",
    "                if loss.requires_grad:\n",
    "                    loss.backward()\n",
    "                return loss\n",
    "            \n",
    "            opt.step(closure)\n",
    "\n",
    "            models[eps].append(model)\n",
    "      \n",
    "    #############################\n",
    "    # estimate treatment effect #\n",
    "    #############################\n",
    "\n",
    "    # initialise e dictionary\n",
    "    e = {}\n",
    "    \n",
    "    # intialise sigma dictionary\n",
    "    sig_d = {}\n",
    "\n",
    "    for eps in epses[1:]:\n",
    "        \n",
    "        # initialise lists for specific eps\n",
    "        pi_hats[eps] = []\n",
    "        e[eps] = []\n",
    "        sig_d[eps] = []\n",
    "        \n",
    "        for i in range(ne):\n",
    "            # get perturbed propensity scores\n",
    "            pi_hats[eps].append(\n",
    "                models[eps][i](X_s1[i]).squeeze()\n",
    "            )\n",
    "                                 \n",
    "            # max of abs of Y1_s1 / propensity score for each experiment\n",
    "            max_abs_Y1_s1_div_ps = torch.max(\n",
    "                torch.abs(Y1_s1[i]) / ((ns - nf) * pi_hats[eps][-1])\n",
    "            )\n",
    "        \n",
    "            # max of abs of Y0_s1 / (1 - propensity score) for each experiment\n",
    "            max_abs_Y0_s1_div_1_m_ps = torch.max(\n",
    "                torch.abs(Y0_s1[i]) / ((ns - nf) * (1 - pi_hats[eps][-1]))\n",
    "            )\n",
    "        \n",
    "            # hstack max_abs_Y_s1_div_ps and max_abs_Y_s1_div_1_m_ps\n",
    "            max_abs_all = torch.stack(\n",
    "                (max_abs_Y1_s1_div_ps, max_abs_Y0_s1_div_1_m_ps)\n",
    "            )\n",
    "        \n",
    "            # replace inf/nan with 1e20 for stability\n",
    "            max_abs_all[torch.isfinite(max_abs_all) == 0] = 1e20\n",
    "\n",
    "            # define sensitivity for estimation\n",
    "            s_e = 2 * torch.max(max_abs_all)\n",
    "\n",
    "            # define sigma for estimation\n",
    "            sigma_e = np.sqrt(\n",
    "                2 * np.log(1.25 / delta) + 1e-10\n",
    "            ) * (s_e / (eps - eps_split[eps][i]))\n",
    "            sig_d[eps].append(sigma_e)\n",
    "\n",
    "            # define e distribution for estimation\n",
    "            e_dist = torch.distributions.normal.Normal(\n",
    "                torch.tensor([0.0], dtype=torch.float64),\n",
    "                sigma_e\n",
    "            )\n",
    "\n",
    "            # draw e for estimation\n",
    "            e[eps].append(e_dist.sample())\n",
    "\n",
    "        # stack lists\n",
    "        pi_hats[eps] = torch.stack(pi_hats[eps])\n",
    "        e[eps] = torch.stack(e[eps]).squeeze()\n",
    "        sig_d[eps] = torch.stack(sig_d[eps]).detach().numpy()\n",
    "    \n",
    "    # get treatment effects\n",
    "    # true\n",
    "    te = {}\n",
    "    # empirical means and std of means of ERM + private ERM\n",
    "    te_hats = {'means': [], 'stds': []}\n",
    "    # means and std of means of privatised te_hats\n",
    "    te_hats_p = {'means': [], 'stds': []}\n",
    "\n",
    "    # estimate true treatment effect\n",
    "    te_ = torch.mean(\n",
    "        Y1_s1 / prob_vec[idx, s1] - Y0_s1 / (1 - prob_vec[idx, s1]),\n",
    "        1,\n",
    "    )\n",
    "    te['mean'] = te_.mean().detach().numpy()\n",
    "    te['std'] = te_.std().detach().numpy()\n",
    "                \n",
    "    for key in pi_hats.keys():\n",
    "        # reduce_mean from (ne, est_split) tensor to (ne, 1) matrix\n",
    "        te_hats_ = torch.mean(\n",
    "            Y1_s1 / pi_hats[key] - Y0_s1 / (1 - pi_hats[key]), \n",
    "            1,\n",
    "        )\n",
    "        te_hats['means'].append(\n",
    "            te_hats_.detach().numpy()\n",
    "        )\n",
    "        te_hats['stds'].append(\n",
    "            te_hats_.std().detach().numpy()\n",
    "        )\n",
    "        try:\n",
    "            te_hats_p_ = te_hats_ + e[key]\n",
    "            te_hats_p['means'].append(\n",
    "                te_hats_p_.detach().numpy()\n",
    "            )\n",
    "            te_hats_p['stds'].append(\n",
    "                te_hats_p_.std().detach().numpy()\n",
    "            )\n",
    "        except KeyError:\n",
    "            # fill first row for later\n",
    "            te_hats_p['means'].append(\n",
    "                te_hats_.detach().numpy()\n",
    "            )\n",
    "            te_hats_p['stds'].append(\n",
    "                te_hats_.std().detach().numpy()\n",
    "            )\n",
    "        \n",
    "    te_hats['means'] = np.array(te_hats['means'])\n",
    "    te_hats['stds'] = np.array(te_hats['stds'])\n",
    "    te_hats_p['means'] = np.array(te_hats_p['means'])\n",
    "    te_hats_p['stds'] = np.array(te_hats_p['stds'])\n",
    "\n",
    "    return te, te_hats, te_hats_p, sig_d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(figname, tau, te_hats, te_hats_p, epses, ne):\n",
    "    '''\n",
    "    plot histogram of empirical probabilities of signs flipping for \\hat{\\tau}_\\epsilon and \\hat{\\tau}_\\epsilon_n\n",
    "    '''\n",
    "    \n",
    "    # deal with nans by setting them to be -bias\n",
    "    for i in range(len(te_hats['means'][1:])):\n",
    "        # \\hat{\\tau}_\\epsilon\n",
    "        te_hats['means'][1:][i][\n",
    "            np.isnan(te_hats['means'][1:][i])\n",
    "        ] = -tau\n",
    "        # \\hat{\\tau}_\\epsilon_n\n",
    "        te_hats_p['means'][1:][i][\n",
    "            np.isnan(te_hats_p['means'][1:][i])\n",
    "        ] = -tau\n",
    "\n",
    "    sgn_tau_hat = np.sign(te_hats['means'][0])\n",
    "\n",
    "    # compute probabilities\n",
    "    probs_te_hats = [\n",
    "        sum(sgn_tau_hat != np.sign(te_hats['means'][1:][i])) / ne\n",
    "        for i in range(len(te_hats['means'][1:]))\n",
    "    ]\n",
    "    \n",
    "    probs_all = [\n",
    "#         sum(abs(sgn_tau_hat + np.sign(te_hats['means'][1:][i]) + np.sign(te_hats_p['means'][1:][i])) != 3) / ne\n",
    "        sum((sgn_tau_hat != np.sign(te_hats['means'][1:][i])).astype('int') + \n",
    "            (sgn_tau_hat != np.sign(te_hats_p['means'][1:][i])).astype('int') == 2) / ne\n",
    "        for i in range(len(te_hats['means'][1:]))\n",
    "    ]\n",
    "    \n",
    "    print(probs_te_hats)\n",
    "    print(probs_all)\n",
    "    \n",
    "    # plot figure\n",
    "    y_name = \"P(sgn($\\\\hat{\\\\tau}_n$) $\\\\neq$ sgn($\\\\hat{\\\\tau}$))\"\n",
    "    y_name_all = \"P(sgn($\\\\hat{\\\\tau}_n^\\\\epsilon$) $\\\\neq$ sgn($\\\\hat{\\\\tau})$, sgn($\\\\hat{\\\\tau}_n$)$\\\\neq$ sgn($\\\\hat{\\\\tau}$))\"\n",
    "        \n",
    "    ind = np.arange(len(epses))\n",
    "    width = 0.35     \n",
    "        \n",
    "    fig, ax = plt.subplots(1,1, figsize=(8, 6))\n",
    "    ax.bar(ind, probs_te_hats, width, color='g', label=y_name)\n",
    "    ax.bar(ind+width, probs_all, width, color='b', label=y_name_all)\n",
    "\n",
    "    ax.set_title(\n",
    "        \"P(sign change) against $\\epsilon$ for $\\\\tau$ = {}\".format(tau),\n",
    "        fontsize=20,\n",
    "    )\n",
    "    ax.set_ylim(0.0, 1.0)\n",
    "    ax.set_xlabel(\"privacy loss ($\\epsilon$)\", fontsize=18)\n",
    "    ax.set_xticks(ind + width / 2)\n",
    "    ax.set_xticklabels([str(i) for i in epses])\n",
    "    ax.tick_params(labelsize=16)\n",
    "    \n",
    "    ax.legend(fontsize=16)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.savefig(figname+'.pdf',dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist_erf(figname, tau, te_hats, te_hats_p, epses, sig_d):\n",
    "    \"\"\"\n",
    "    plot histogram of empirical probabilities of signs flipping for \\hat{\\tau}_n^\\epsilon w.r.t erf(|\\hat{\\tau}_n}| / sigma_n)\n",
    "    \"\"\"\n",
    "\n",
    "    # deal with nans by setting them to be -bias\n",
    "    for i in range(len(te_hats[\"means\"][1:])):\n",
    "        # \\hat{\\tau}_n\n",
    "        te_hats[\"means\"][1:][i][\n",
    "            np.isnan(te_hats[\"means\"][1:][i])\n",
    "        ] = -tau\n",
    "        # \\hat{\\tau}_n^\\epsilon\n",
    "        te_hats_p[\"means\"][1:][i][\n",
    "            np.isnan(te_hats_p[\"means\"][1:][i])\n",
    "        ] = -tau\n",
    "\n",
    "    sgn_tau_hat = np.sign(te_hats[\"means\"][0])\n",
    "\n",
    "    sgn_list = []\n",
    "    val_list = []\n",
    "\n",
    "    for i in range(len(epses)):\n",
    "        # get sigma\n",
    "        sigma = sig_d[epses[i]]\n",
    "\n",
    "        # get sgn and val for eps whose sgn(\\hat{\\tau}_n) != sgn(\\hat{\\tau})\n",
    "        sgn_flip_idx = (\n",
    "            np.sign(te_hats[\"means\"][i + 1]) != sgn_tau_hat\n",
    "        )\n",
    "\n",
    "        # get tau_hat_n of disagreements\n",
    "        sgn_flip_te_hat = te_hats[\"means\"][i + 1][\n",
    "            sgn_flip_idx\n",
    "        ]\n",
    "\n",
    "        abs_sgn_flip_te_hat_div_sigma = (\n",
    "            np.abs(sgn_flip_te_hat) / sigma[sgn_flip_idx]\n",
    "        )\n",
    "\n",
    "        sgn_list += (\n",
    "            np.sign(te_hats_p[\"means\"][i + 1][sgn_flip_idx])\n",
    "            != sgn_tau_hat[sgn_flip_idx]\n",
    "        ).tolist()\n",
    "\n",
    "        # add all val where sgn_tau_hat_n != sgn_tau_hat\n",
    "        val_list += abs_sgn_flip_te_hat_div_sigma.tolist()\n",
    "\n",
    "    sgn_list = np.array(sgn_list)\n",
    "    val_list = np.array(val_list)\n",
    "\n",
    "    # set # of bins\n",
    "    bins = np.linspace(0, 0.40, 20)\n",
    "\n",
    "    # bin w.r.t to vals where sgn(\\hat{\\tau}_n^\\epsilon) != sgn(\\hat{\\tau})\n",
    "    hist1, bins1 = np.histogram(\n",
    "        erf(val_list[sgn_list == 1]), bins=bins\n",
    "    )\n",
    "\n",
    "    binWidth1 = bins1[1] - bins1[0]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "\n",
    "    lab_1 = (\n",
    "        \"# of sgn($\\\\hat{\\\\tau}_n$) $\\\\neq$ sgn($\\\\hat{\\\\tau}) = $\"\n",
    "        + str(len(val_list))\n",
    "    )\n",
    "    lab_2 = (\n",
    "        \"# of sgn($\\\\hat{\\\\tau}_n^\\\\epsilon$) $\\\\neq$ sgn($\\\\hat{\\\\tau}) = $\"\n",
    "        + str(len(val_list[sgn_list == 1]))\n",
    "    )\n",
    "\n",
    "    ax.bar(\n",
    "        bins1[:-1],\n",
    "        (\n",
    "            hist1 / len(val_list)\n",
    "        ),  # divide by number of times where sgn(\\hat{\\tau}_n) != sgn(\\hat{\\tau}) to get probability\n",
    "        binWidth1,\n",
    "        alpha=0.7,\n",
    "    )\n",
    "\n",
    "    ax.set_title(\n",
    "        \"P(sgn($\\\\hat{\\\\tau}_n^\\\\epsilon$) $\\\\neq$ sgn($\\\\hat{\\\\tau}$) | sgn($\\\\hat{\\\\tau}_n$) $\\\\neq$ sgn($\\\\hat{\\\\tau}$)) for $\\\\tau$ = \"\n",
    "        + str(tau),\n",
    "        fontsize=20,\n",
    "    )\n",
    "    ax.tick_params(labelsize=16)\n",
    "    ax.set_xlabel(\n",
    "        \"erf(|$\\\\hat{\\\\tau}_n$|/$\\\\sigma_n$)\", fontsize=18\n",
    "    )\n",
    "\n",
    "    empty_leg = Rectangle((0, 0), 0, 0, alpha=0.0)\n",
    "    ax.legend(\n",
    "        [empty_leg, empty_leg],\n",
    "        [lab_1, lab_2],\n",
    "        handlelength=0,\n",
    "        fontsize=14,\n",
    "        frameon=False,\n",
    "    )\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(figname + \"_\" + str(tau) + \".pdf\", dpi=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_te(figname, te, te_hats, te_hats_analytic, epses, eps_pos, tau,):\n",
    "    '''\n",
    "    plot the true treatment effect, ERM, private ERM treatment effect\n",
    "    \n",
    "    eps_pos is the position of the first eps to plot frrom\n",
    "    '''\n",
    "\n",
    "    # get means and stds\n",
    "    te_hat = np.mean([te_hats['means'][0]], 1)\n",
    "    te_hat_std = np.std([te_hats['means'][0]])\n",
    "    te_hat_z = np.mean(te_hats['means'][1:], 1)[eps_pos:]\n",
    "    te_hat_z_std = np.std(te_hats['means'][1:], 1)[eps_pos:]\n",
    "    te_hat_mu = np.mean(te_hats_analytic['means'], 1)[eps_pos:]\n",
    "    te_hat_mu_std = (\n",
    "        np.std(te_hats_analytic['means'], 1)[eps_pos:] + np.mean(te_hats_analytic['stds'], 1)[eps_pos:]\n",
    "    )\n",
    "\n",
    "    # plot figure\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "\n",
    "    ax.plot(\n",
    "        epses[eps_pos:],\n",
    "        [te['mean']] * len(epses[eps_pos:]),\n",
    "        marker='o',\n",
    "        color='magenta',\n",
    "        label=\"Truth\",\n",
    "    )\n",
    "    ax.plot(\n",
    "        epses[eps_pos:],\n",
    "        [te_hat] * len(epses[eps_pos:]),\n",
    "        marker='o',\n",
    "        color='red',\n",
    "        label=\"ERM\",\n",
    "    )\n",
    "    ax.plot(\n",
    "        epses[eps_pos:],\n",
    "        te_hat_z,\n",
    "        marker='x',\n",
    "        color='blue',\n",
    "        label=\"Empirical Private ERM\",\n",
    "    )\n",
    "    ax.plot(\n",
    "        epses[eps_pos:],\n",
    "        te_hat_mu,\n",
    "        marker='d',\n",
    "        color='green',\n",
    "        label=\"Analytical Private ERM\",\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        epses[eps_pos:],\n",
    "        te_hat + te_hat_std,\n",
    "        te_hat - te_hat_std,\n",
    "        facecolor='red',\n",
    "        alpha=0.25,\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        epses[eps_pos:],\n",
    "        te_hat_z + te_hat_z_std,\n",
    "        te_hat_z - te_hat_z_std,\n",
    "        facecolor='blue',\n",
    "        alpha=0.25,\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        epses[eps_pos:],\n",
    "        te_hat_mu + te_hat_mu_std,\n",
    "        te_hat_mu - te_hat_mu_std,\n",
    "        facecolor='green',\n",
    "        alpha=0.25,\n",
    "    )\n",
    "\n",
    "    ax.set_title(\n",
    "        \"True, ERM, Empirical Private ERM and Analytical Private ERM ATE against $\\epsilon$\",\n",
    "        fontsize=20,\n",
    "    )\n",
    "    ax.set_xlabel(\"$\\epsilon$\", fontsize=18)\n",
    "    # set legend position\n",
    "    if tau > 0:\n",
    "        ax.legend(fontsize=16, loc=1)\n",
    "    else:\n",
    "        ax.legend(fontsize=16, loc=4)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(figname + '.png', dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\tau = 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sikai/.pyenv/versions/3.7.6/lib/python3.7/site-packages/ipykernel_launcher.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "te, te_hats, te_hats_p, sig_d = IPW_PPS_Out(X, T, prob_vec, Y, tau, epses, delta, reg_co, nf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.366, 0.29, 0.114, 0.064, 0.024, 0.006, 0.002]\n",
      "[0.19, 0.148, 0.054, 0.028, 0.014, 0.004, 0.002]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_hist('sim_tau_'+str(tau)+'_epses_flip_prob_out_new', tau, te_hats, te_hats_p, epses[1:], ne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "te, te_hats, te_hats_p, sig_d = IPW_PPS_Obj(X, T, prob_vec, Y, tau, epses, delta, reg_co, nf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist('sim_tau_'+str(tau)+'_epses_flip_prob_obj_new', tau, te_hats, te_hats_p, epses[1:], ne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_hist_erf('hist_erf', tau, te_hats, te_hats_p, epses, sig_d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
