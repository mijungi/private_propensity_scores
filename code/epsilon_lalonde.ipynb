{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of $\\epsilon$ on Average Treatment Effect on LaLonde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12971f5b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from agm import calibrateAnalyticGaussianMechanism\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# set random seed\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no. experiments, no. samples for fitting, no. samples for estimating, no. of draws of z\n",
    "ne = 1000\n",
    "nf = 500\n",
    "nt = 200\n",
    "nd = 1\n",
    "\n",
    "# privacy parameters\n",
    "epses = [0.2, 0.4, 0.6, 0.8, 0.99]\n",
    "delta = 1. / (nf ** 2)\n",
    "\n",
    "# regularisation coefficient\n",
    "reg_co = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess LaLonde data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data and remove index and rename last column\n",
    "lalonde_df = (\n",
    "    pd.read_csv('data/LaLonde-CBPS/lalonde.csv')\n",
    "    .drop('Unnamed: 0', 1)\n",
    "    .rename(columns={'re74.miss': 're74_miss'})\n",
    ")\n",
    "\n",
    "# remove points not in original LaLonde dataset and drop exper\n",
    "lalonde_df = (\n",
    "    lalonde_df[lalonde_df.exper == 1]\n",
    "#     lalonde_df[lalonde_df.re74_miss == 0]\n",
    "    .drop(['exper'], 1)\n",
    "#     .drop(['exper', 're74_miss'], 1)\n",
    ")\n",
    "\n",
    "# change column positions\n",
    "cols = list(lalonde_df.columns)\n",
    "cols = cols[:-2] + [cols[-1]] + [cols[-2]]\n",
    "lalonde_df = lalonde_df[cols]\n",
    "\n",
    "# use dict structure\n",
    "lalonde = {}\n",
    "\n",
    "# get X, T, Y\n",
    "# restrict X to ||x||_2 \\leq 1 to fit assumption\n",
    "X = torch.tensor(lalonde_df.iloc[:, 1:-1].values, dtype=torch.float64)\n",
    "lalonde['x'] = X / X.norm(dim=1).max()\n",
    "lalonde['t'] = torch.tensor(lalonde_df['treat'].values, dtype=torch.float64)\n",
    "lalonde['y'] = torch.tensor(lalonde_df['re78'].values, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get no. experiments and dim\n",
    "_, d = lalonde['x'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate X_test, T_test, Y_test through subsampling\n",
    "Y_test, X_test = [], []\n",
    "T_test = torch.stack([torch.cat([torch.ones(int(nt/2), dtype=torch.float64), torch.zeros(int(nt/2), dtype=torch.float64)])] * ne)\n",
    "\n",
    "Y_train, X_train = [], []\n",
    "T_train = torch.stack([torch.cat([torch.ones(int(nf/2), dtype=torch.float64), torch.zeros(int(nf/2), dtype=torch.float64)])] * ne)\n",
    "\n",
    "for i in range(ne):\n",
    "    # get indices for t=1 and t=0\n",
    "    t1_idx = lalonde['t'].nonzero().squeeze().numpy()\n",
    "    t0_idx = (1 - lalonde['t']).nonzero().squeeze().numpy()\n",
    "                 \n",
    "    # subsample without replacement nt indices, nt/2 for t=1 and nt/2 for t=0\n",
    "    sam_t1, sam_t0 = np.random.choice(t1_idx, int(nt/2), replace=False), np.random.choice(t0_idx, int(nt/2), replace=False)\n",
    "    sam_idx = np.hstack([sam_t1, sam_t0])\n",
    "\n",
    "    # subsample with replacement nf indices, nf/2 for t=1 and nf/2 for t=0\n",
    "    unsam_t1, unsam_t0 = np.setxor1d(t1_idx, sam_t1), np.setxor1d(t0_idx, sam_t0)\n",
    "    unsam_idx = np.hstack([np.random.choice(unsam_t1, int(nf/2), replace=True), np.random.choice(unsam_t0, int(nf/2), replace=True)])\n",
    "    \n",
    "    Y_test.append(lalonde['y'][sam_idx])\n",
    "    X_test.append(lalonde['x'][sam_idx, :])\n",
    "\n",
    "    Y_train.append(lalonde['y'][unsam_idx])\n",
    "    X_train.append(lalonde['x'][unsam_idx, :])\n",
    "\n",
    "# convert to torch tensors \n",
    "Y_test, X_test = torch.stack(Y_test), torch.stack(X_test)\n",
    "Y_train, X_train = torch.stack(Y_train), torch.stack(X_train)\n",
    "\n",
    "# permute data\n",
    "# permute indices\n",
    "perm_test = torch.stack([torch.randperm(nt) for i in range(ne)])\n",
    "perm_train = torch.stack([torch.randperm(nf) for i in range(ne)])\n",
    "\n",
    "# create auxiliary indices\n",
    "idx = torch.arange(ne)[:, None]\n",
    "\n",
    "# permute X, T, Y\n",
    "X_all = torch.cat([X_train[idx, perm_train], X_test[idx, perm_test]], 1) \n",
    "T_all = torch.cat([T_train[idx, perm_train], T_test[idx, perm_test]], 1) \n",
    "Y_all = torch.cat([Y_train[idx, perm_train], Y_test[idx, perm_test]], 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model and method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Log_Reg(torch.nn.Module):\n",
    "    '''\n",
    "    Logistic Regression\n",
    "    '''\n",
    "    def __init__(self, D_in, D_out):\n",
    "        super(Log_Reg, self).__init__()\n",
    "        self.linear = torch.nn.Linear(D_in, D_out, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = torch.sigmoid(self.linear(x))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPTW_PPS(X, T, Y, bias, epses, delta, reg_co, nd, nf):\n",
    "    '''\n",
    "    average treatment effect with inverse probability of treatment weighting using private propensity scores\n",
    "    '''\n",
    "    # get # experiments, # samples, # dimensions\n",
    "    ne, ns, dim = X.shape\n",
    "    \n",
    "    # sgd step size\n",
    "    step_size = 0.01\n",
    "    \n",
    "    ################\n",
    "    # process data #\n",
    "    ################\n",
    "    \n",
    "    # get Y0 and Y1\n",
    "    Y0 = Y * (1 - T)\n",
    "    Y1 = (Y + bias) * T\n",
    "    \n",
    "    # split data\n",
    "    # get splits\n",
    "    fit_split = nf\n",
    "    est_split = ns - nf\n",
    "\n",
    "    # permute indices\n",
    "    perm = torch.stack([torch.randperm(ns) for i in range(ne)])\n",
    "\n",
    "    # create auxiliary indices\n",
    "    idx = torch.arange(ne)[:, None]\n",
    "\n",
    "    # split X into fit, estimate splits\n",
    "    X_s0 = X[:, :fit_split]\n",
    "    X_s1 = X[:, fit_split:]\n",
    "\n",
    "    # expand dim of T to allow multiplication with X\n",
    "    T_ex_dim = T.reshape(ne, ns, 1)\n",
    "\n",
    "    # split X0 and X1 into fit, estimate splits\n",
    "    X0_s1 = (X * (1- T_ex_dim))[:, fit_split:]\n",
    "    X1_s1 = (X * T_ex_dim)[:, fit_split:]\n",
    "\n",
    "    # precompute ||X0_sl||_2^2 and ||X1_sl||_2^2\n",
    "    X0_s1_2 = X0_s1.norm(dim=2) ** 2\n",
    "    X1_s1_2 = X1_s1.norm(dim=2) ** 2\n",
    "\n",
    "    # add x_i to x_j for X0_plus_X0_s1 and X1_plus_X1_s1\n",
    "    X0_plus_X0_s1 = X0_s1.reshape(ne, est_split, 1, dim) + X0_s1.reshape(ne, 1, est_split, dim)\n",
    "    X1_plus_X1_s1 = X1_s1.reshape(ne, est_split, 1, dim) + X1_s1.reshape(ne, 1, est_split, dim)\n",
    "\n",
    "    # get norms squared for X0_plus_X0_s1 and X1_plus_X1_sl\n",
    "    X0_plus_X0_sl_norm_2 = X0_plus_X0_s1.norm(dim=-1) ** 2 \n",
    "    X1_plus_X1_sl_norm_2 = X1_plus_X1_s1.norm(dim=-1) ** 2 \n",
    "\n",
    "    # split T into fit, estimate splits\n",
    "    T_s0 = T[:, :fit_split]\n",
    "    T_s1 = T[:, fit_split:]\n",
    "\n",
    "    # split Y0 and Y1 into fit, estimate splits \n",
    "    Y0_s0 = Y0[:, :fit_split] \n",
    "    Y1_s0 = Y1[:, :fit_split] \n",
    "\n",
    "    Y0_s1 = Y0[:, fit_split:] \n",
    "    Y1_s1 = Y1[:, fit_split:] \n",
    "\n",
    "    # mult y_i to y_j for Y0_times_Y0_s1 and Y1_times_Y1_s1\n",
    "    Y0_times_Y0_s1 = Y0_s1.reshape(ne, est_split, 1) * Y0_s1.reshape(ne, 1, est_split)\n",
    "    Y1_times_Y1_s1 = Y1_s1.reshape(ne, est_split, 1) * Y1_s1.reshape(ne, 1, est_split)\n",
    "    \n",
    "    # reshape estimate splits for later\n",
    "    Y0_s1 = Y0_s1.reshape(ne, 1, est_split)\n",
    "    Y1_s1 = Y1_s1.reshape(ne, 1, est_split)\n",
    "    \n",
    "    ##############\n",
    "    # fit models #\n",
    "    ##############\n",
    "\n",
    "    # instantiate ne different models\n",
    "    models = [Log_Reg(dim, 1) for i in range(ne)]\n",
    "    # set model parameters to float64\n",
    "    [model.double() for model in models]\n",
    "\n",
    "    # define loss (binary cross entropy)\n",
    "    loss = torch.nn.BCELoss()\n",
    "\n",
    "    # define optimisers\n",
    "    optimisers = [torch.optim.SGD(models[i].parameters(), lr=step_size, weight_decay=reg_co) for i in range(ne)]\n",
    "    \n",
    "    # train models\n",
    "    for t in range(1000):\n",
    "        preds = [models[i](X_s0[i]).squeeze() for i in range(ne)]\n",
    "        losses = [loss(preds[i], T_s0[i]) for i in range(ne)]\n",
    "        [opt.zero_grad for opt in optimisers]\n",
    "        [loss.backward() for loss in losses]\n",
    "        [opt.step() for opt in optimisers]  \n",
    "        \n",
    "    #############################    \n",
    "    # estimate treatment effect #\n",
    "    #############################\n",
    "    \n",
    "    # initialise pi_hat dictionaries \n",
    "    pi_hats = {} \n",
    "    pi_hats_analytic = {}\n",
    "    \n",
    "    # get estimated propensity scores\n",
    "    pi_hats[0] = torch.stack([models[i](X_s1[i]).squeeze() for i in range(ne)])\n",
    "\n",
    "    # perturb model and get relevant quantities\n",
    "    for eps in epses:\n",
    "        # define sigma\n",
    "        s_a = 2. / (fit_split * reg_co)\n",
    "\n",
    "        # gaussian mechanism\n",
    "        sigma = np.sqrt(2 * np.log(1.25 / delta) + 1e-10) *  (s_a  / eps)\n",
    "        sigma_2 = sigma ** 2\n",
    "\n",
    "        # # analytic gaussian mechanism\n",
    "        # sigma = calibrateAnalyticGaussianMechanism(eps, delta, s_a)\n",
    "        # sigma_2 = sigma ** 2\n",
    "\n",
    "        # define noise distribution\n",
    "        noise_dist = torch.distributions.normal.Normal(torch.tensor([0.0], dtype=torch.float64), torch.tensor([sigma], dtype=torch.float64))\n",
    "\n",
    "        # draw noise vectors\n",
    "        noise_vecs = noise_dist.sample((ne, nd, dim)).reshape(ne, nd, dim)\n",
    "        \n",
    "        # create temp models \n",
    "        models_ = [copy.deepcopy(models) for i in range(nd)]\n",
    "        \n",
    "        # \\hat{\\w}^\\top Xs\n",
    "        w_T_X0_s1 = []\n",
    "        w_T_X0_plus_X0_s1 = []\n",
    "        w_T_X1_s1 = []\n",
    "        w_T_X1_plus_X1_s1 = []\n",
    "\n",
    "        # initialise list for privatised estimated propensity scores\n",
    "        pi_hats[eps] = []\n",
    "        \n",
    "        # perturb weights with noise vectors\n",
    "        for i in range(ne):\n",
    "            w_T_X0_s1.append(torch.einsum('ij,kj-> i', X0_s1[i], models[i].linear.weight))\n",
    "            w_T_X0_plus_X0_s1.append(torch.einsum('ijk,lk-> ij', X0_plus_X0_s1[i], models[i].linear.weight))\n",
    "            w_T_X1_s1.append(torch.einsum('ij,kj-> i', X1_s1[i], models[i].linear.weight))\n",
    "            w_T_X1_plus_X1_s1.append(torch.einsum('ijk,lk-> ij', X1_plus_X1_s1[i], models[i].linear.weight))\n",
    "            for j in range(nd):\n",
    "                model_temp = models_[j][i]\n",
    "                model_temp.linear.weight.data.add_(noise_vecs[i, j, :])\n",
    "                pi_hats[eps].append(model_temp(X_s1[i]).squeeze())\n",
    "                \n",
    "        # reshape stacked privatised estimated propensity scores as ne * nd\n",
    "        pi_hats[eps] = torch.stack(pi_hats[eps]).reshape(ne, nd, est_split)\n",
    "        \n",
    "        # precompute sigma^2 ||x||_2^2 and \\w^\\top Xs\n",
    "        pi_hats_analytic[eps] = {'sigma_2_X0_s1_2': sigma_2 * X0_s1_2}\n",
    "        pi_hats_analytic[eps]['sigma_2_X1_s1_2'] = sigma_2 * X1_s1_2\n",
    "        pi_hats_analytic[eps]['sigma_2_X0_plus_X0_s1'] = sigma_2 * X0_plus_X0_sl_norm_2\n",
    "        pi_hats_analytic[eps]['sigma_2_X1_plus_X1_s1'] = sigma_2 * X1_plus_X1_sl_norm_2\n",
    "        pi_hats_analytic[eps]['w_T_X0_s1'] = torch.stack(w_T_X0_s1)\n",
    "        pi_hats_analytic[eps]['w_T_X0_plus_X0_s1'] = torch.stack(w_T_X0_plus_X0_s1)\n",
    "        pi_hats_analytic[eps]['w_T_X1_s1'] = torch.stack(w_T_X1_s1)\n",
    "        pi_hats_analytic[eps]['w_T_X1_plus_X1_s1'] = torch.stack(w_T_X1_plus_X1_s1)\n",
    "                \n",
    "    # get treatment effects\n",
    "    # empirical means and stds of means of ERM + noise \n",
    "    te_hats = {'means': [], 'stds': []}\n",
    "    # analytic means and stds of ERM + noise\n",
    "    te_hats_analytic = {'means': [], 'stds': []}\n",
    "\n",
    "    for key in pi_hats.keys():\n",
    "        if key != 0:\n",
    "            # empirical estimates\n",
    "            # reduce_mean from (ne, nd, est_split) tensor to (ne * nd, 1) matrix\n",
    "            te_hats_ = torch.mean(Y1_s1 / pi_hats[key] - Y0_s1 / (1 - pi_hats[key]), 2)\n",
    "            # analytic estimates\n",
    "            # expectation and variance of mu_0\n",
    "            rand_mu_0 = Y0_s1.squeeze() * torch.exp(pi_hats_analytic[key]['w_T_X0_s1'] + pi_hats_analytic[key]['sigma_2_X0_s1_2']/2)\n",
    "            E_mu_0 = torch.mean(Y0_s1.squeeze() + rand_mu_0, [1])\n",
    "            mu_X0_s1_2 = Y0_times_Y0_s1 * torch.exp(pi_hats_analytic[key]['w_T_X0_plus_X0_s1'] + pi_hats_analytic[key]['sigma_2_X0_plus_X0_s1']/2)\n",
    "            mu_X0_s1_mu_X0_s1 = rand_mu_0.reshape(ne, 1, est_split) * rand_mu_0.reshape(ne, est_split, 1)\n",
    "            var_mu_0 = torch.mean(mu_X0_s1_2 - mu_X0_s1_mu_X0_s1, [1, 2])\n",
    "            # expectation and variance of mu_1\n",
    "            rand_mu_1 = Y1_s1.squeeze() * torch.exp(- pi_hats_analytic[key]['w_T_X1_s1'] + pi_hats_analytic[key]['sigma_2_X1_s1_2']/2)\n",
    "            E_mu_1 = torch.mean(Y1_s1.squeeze() + rand_mu_1, [1])\n",
    "            mu_X1_s1_2 = Y1_times_Y1_s1 * torch.exp(- pi_hats_analytic[key]['w_T_X1_plus_X1_s1'] + pi_hats_analytic[key]['sigma_2_X1_plus_X1_s1']/2)\n",
    "            mu_X1_s1_mu_X1_s1 = rand_mu_1.reshape(ne, 1, est_split) * rand_mu_1.reshape(ne, est_split, 1)\n",
    "            var_mu_1 = torch.mean(mu_X1_s1_2 - mu_X1_s1_mu_X1_s1, [1, 2])\n",
    "            # expectation and variance of te_hats\n",
    "            te_hats_analytic_mu = E_mu_1 - E_mu_0\n",
    "            te_hats_analytic_std = torch.sqrt(var_mu_1 + var_mu_0)\n",
    "            te_hats_analytic['means'].append(te_hats_analytic_mu.detach().numpy())\n",
    "            te_hats_analytic['stds'].append(te_hats_analytic_std.detach().numpy())\n",
    "        else:\n",
    "            # empirical estimate for noiseless case\n",
    "            # reduce_mean from (ne, est_split) tensor to (ne , 1) matrix\n",
    "            te_hats_ = torch.mean(Y1_s1.squeeze() / pi_hats[key] - Y0_s1.squeeze() / (1 - pi_hats[key]), 1).reshape(ne, 1)\n",
    "        te_hats['means'].append([te_hats_[i].mean().detach().numpy() for i in range(ne)])\n",
    "        te_hats['stds'].append([te_hats_[i].std().detach().numpy() for i in range(ne)])            \n",
    "    \n",
    "    te_hats['means'] = np.array(te_hats['means'])\n",
    "    te_hats['stds'] = np.array(te_hats['stds'])\n",
    "    te_hats_analytic['means'] = np.array(te_hats_analytic['means'])\n",
    "    te_hats_analytic['stds'] = np.array(te_hats_analytic['stds'])\n",
    "    \n",
    "    return te_hats, te_hats_analytic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_te(figname, te_hats, te_hats_analytic, epses, plot_std=1):\n",
    "    '''\n",
    "    plot the ERM, private ERM treatment effect\n",
    "    '''\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1, figsize=(20, 10))\n",
    "\n",
    "    ax.plot(epses, [te_hats['mean'][0]] * len(epses) , marker='o', color='red', label=\"ERM\")\n",
    "    ax.plot(epses, te_hats['mean'][1:], marker='x', color='blue', label=\"Empirical Private ERM\")\n",
    "    ax.plot(epses,te_hats_analytic['mean'],marker='d',color='green',label=\"Analytical Private ERM\")\n",
    "    if plot_std:\n",
    "        ax.fill_between(epses, te_hats['mean'][1:] + te_hats['std_mean'][1:], te_hats['mean'][1:] - te_hats['std_mean'][1:], facecolor='blue', alpha=0.25)\n",
    "#         ax.fill_between(epses,te_hats_analytic['mean']+te_hats_analytic['std_max'],te_hats_analytic['mean']-te_hats_analytic['std_max'], facecolor='green', alpha=0.25)\n",
    "        ax.fill_between(epses,te_hats_analytic['mean']+te_hats_analytic['std_mean'],te_hats_analytic['mean']-te_hats_analytic['std_mean'], facecolor='yellow', alpha=0.25)\n",
    "\n",
    "    ax.set_title(\"ERM, Empirical Private ERM and Analytical Private ERM of Average Treatment Effect\", fontsize=16)\n",
    "    ax.set_ylabel(\"Average Treatment Effect\", fontsize=16)\n",
    "    ax.set_xlabel(\"$\\epsilon$\", fontsize=16)\n",
    "    ax.legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(figname + '.png',dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run method and plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_hats, te_hats_analytic = IPTW_PPS(X_all, T_all, Y_all, 0, epses, delta, reg_co, nd, nf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.143, 0.068, 0.05, 0.03, 0.028]\n"
     ]
    }
   ],
   "source": [
    "sgn_tau_hat = np.sign(te_hats['means'][0])\n",
    "    \n",
    "# compute probabilities\n",
    "probs = [sum(np.sign(i) != sgn_tau_hat) / ne for i in te_hats['means'][1:]]\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[902.1086545150501, 872.2163852736154, 892.2209852702304, 904.9505716458677, 898.8150398526142, 904.0730387357378]\n"
     ]
    }
   ],
   "source": [
    "print([np.mean(i) for i in te_hats['means']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[748.0731260273692, 1079.881049813299, 823.7611911650139, 773.8093998860544, 772.7444643545851, 766.6388137329893]\n"
     ]
    }
   ],
   "source": [
    "print([np.std(i) for i in te_hats['means']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
