{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of $m$ on Average Treatment Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11550d970>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from misc.agm import calibrateAnalyticGaussianMechanism\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# set random seed\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no. experiments, no. draws of z, no. samples, dim, X\n",
    "ne = 100\n",
    "nd = 1\n",
    "ns = [1500, 2000, 2500, 3000, 3500]\n",
    "dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no. of points used to fit log reg\n",
    "nf = [500, 1000, 1500, 2000, 2500]\n",
    "\n",
    "# privacy parameters\n",
    "epses = [0.1, 0.3, 0.5]\n",
    "delta = 1e-6\n",
    "\n",
    "# regularisation coefficient\n",
    "reg_co = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectors for generating Y, differentiate between Y_0 and Y_1 with true treatment effect tau\n",
    "# Y = beta^T X + 0.1 Z\n",
    "# Y_1 = Y + tau, Y_0 = Y\n",
    "beta_std = 1\n",
    "beta_dist = torch.distributions.normal.Normal(\n",
    "    torch.tensor([0], dtype=torch.float64), \n",
    "    torch.tensor([beta_std], dtype=torch.float64)\n",
    ")\n",
    "beta = beta_dist.sample((dim, 1)).reshape(dim, 1)\n",
    "\n",
    "# true treatment effect tau\n",
    "tau = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectors for generating T\n",
    "# T = exp(-T_w^T X + b)\n",
    "T_std = 1\n",
    "T_dist = torch.distributions.normal.Normal(\n",
    "    torch.tensor([0.0], dtype=torch.float64), \n",
    "    torch.tensor([T_std], dtype=torch.float64)\n",
    ")\n",
    "T_w = T_dist.sample((dim, 1)).reshape(dim, 1)\n",
    "T_b = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Log_Reg(torch.nn.Module):\n",
    "    '''\n",
    "    Logistic Regression\n",
    "    '''\n",
    "    def __init__(self, D_in, D_out):\n",
    "        super(Log_Reg, self).__init__()\n",
    "        self.linear = torch.nn.Linear(D_in, D_out, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = torch.sigmoid(self.linear(x))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPTW_PPS(\n",
    "    X, T, prob_vec, Y, tau, epses, delta, reg_co, nd, nf\n",
    "):\n",
    "    '''\n",
    "    average treatment effect with inverse probability of treatment weighting using private propensity scores\n",
    "    '''\n",
    "    # get # experiments, # samples, # dimensions\n",
    "    ne, ns, dim = X.shape\n",
    "\n",
    "    # sgd step size\n",
    "    step_size = 0.01\n",
    "\n",
    "    ################\n",
    "    # process data #\n",
    "    ################\n",
    "\n",
    "    # get Y0 and Y1\n",
    "    Y0 = Y * (1 - T)\n",
    "    Y1 = (Y + tau) * T\n",
    "\n",
    "    # split data\n",
    "    # get splits\n",
    "    fit_split = nf\n",
    "    est_split = ns - nf\n",
    "\n",
    "    # permute indices\n",
    "    perm = torch.stack(\n",
    "        [torch.randperm(ns) for i in range(ne)]\n",
    "    )\n",
    "\n",
    "    # create splits\n",
    "    s0 = perm[:, :fit_split]\n",
    "    s1 = perm[:, fit_split:]\n",
    "\n",
    "    # create auxiliary indices\n",
    "    idx = torch.arange(ne)[:, None]\n",
    "\n",
    "    # split X into fit, estimate splits\n",
    "    X_s0 = X[idx, s0]\n",
    "    X_s1 = X[idx, s1]\n",
    "\n",
    "    # expand dim of T to allow multiplication with X\n",
    "    T_ex_dim = T.reshape(ne, ns, 1)\n",
    "\n",
    "    # split X0 and X1 into fit, estimate splits\n",
    "    X0_s1 = (X * (1 - T_ex_dim))[idx, s1]\n",
    "    X1_s1 = (X * T_ex_dim)[idx, s1]\n",
    "\n",
    "    # split T into fit, estimate splits\n",
    "    T_s0 = T[idx, s0]\n",
    "    T_s1 = T[idx, s1]\n",
    "\n",
    "    # split Y0 and Y1 into fit, estimate splits\n",
    "    Y0_s0 = Y0[idx, s0]\n",
    "    Y1_s0 = Y1[idx, s0]\n",
    "\n",
    "    Y0_s1 = Y0[idx, s1]\n",
    "    Y1_s1 = Y1[idx, s1]\n",
    "\n",
    "    # reshape estimate splits for later\n",
    "    Y0_s1 = Y0_s1.reshape(ne, 1, est_split)\n",
    "    Y1_s1 = Y1_s1.reshape(ne, 1, est_split)\n",
    "    \n",
    "    ##############\n",
    "    # fit models #\n",
    "    ##############\n",
    "\n",
    "    # instantiate ne different models\n",
    "    models = [Log_Reg(dim, 1) for i in range(ne)]\n",
    "    # set model parameters to float64\n",
    "    [model.double() for model in models]\n",
    "\n",
    "    # define loss (binary cross entropy)\n",
    "    loss = torch.nn.BCELoss()\n",
    "\n",
    "    # define optimisers\n",
    "    optimisers = [\n",
    "        torch.optim.SGD(\n",
    "            models[i].parameters(),\n",
    "            lr=step_size,\n",
    "            weight_decay=reg_co,\n",
    "        )\n",
    "        for i in range(ne)\n",
    "    ]\n",
    "\n",
    "    # train models\n",
    "    for t in range(1000):\n",
    "        preds = [\n",
    "            models[i](X_s0[i]).squeeze() for i in range(ne)\n",
    "        ]\n",
    "        losses = [\n",
    "            loss(preds[i], T_s0[i]) for i in range(ne)\n",
    "        ]\n",
    "        [opt.zero_grad for opt in optimisers]\n",
    "        [loss.backward() for loss in losses]\n",
    "        [opt.step() for opt in optimisers]\n",
    "\n",
    "    #############################\n",
    "    # estimate treatment effect #\n",
    "    #############################\n",
    "\n",
    "    # initialise pi_hat dictionaries\n",
    "    pi_hats = {}\n",
    "    \n",
    "    # initialise e dictionary\n",
    "    e = {}\n",
    "\n",
    "    # get estimated propensity scores\n",
    "    pi_hats[0] = torch.stack(\n",
    "        [models[i](X_s1[i]).squeeze() for i in range(ne)]\n",
    "    )\n",
    "\n",
    "    # perturb model and get relevant quantities\n",
    "    for eps in epses:\n",
    "        # define sensitivity for log reg\n",
    "        s_w = 2.0 / (fit_split * reg_co)\n",
    "\n",
    "        # define sigma for log reg\n",
    "        sigma = np.sqrt(\n",
    "            2 * np.log(1.25 / delta) + 1e-10\n",
    "        ) * (s_w / eps)\n",
    "        sigma_2 = sigma ** 2\n",
    "\n",
    "#         # analytic gaussian mechanism\n",
    "#         sigma = calibrateAnalyticGaussianMechanism(eps, delta, s_w)\n",
    "#         sigma_2 = sigma ** 2\n",
    "\n",
    "        # define z distribution for log reg\n",
    "        z_dist = torch.distributions.normal.Normal(\n",
    "            torch.tensor([0.0], dtype=torch.float64),\n",
    "            torch.tensor([sigma], dtype=torch.float64),\n",
    "        )\n",
    "\n",
    "        # draw z for log reg\n",
    "        z_vecs = z_dist.sample(\n",
    "            (ne, nd, dim)\n",
    "        ).reshape(ne, nd, dim)\n",
    "\n",
    "        # create temp models\n",
    "        models_ = [copy.deepcopy(models) for i in range(nd)]\n",
    "\n",
    "        # initialise list for privatised estimated propensity scores\n",
    "        pi_hats[eps] = []\n",
    "\n",
    "        # perturb weights with z_vecs\n",
    "        for i in range(ne):\n",
    "            for j in range(nd):\n",
    "                model_temp = models_[j][i]\n",
    "                model_temp.linear.weight.data.add_(\n",
    "                    z_vecs[i, j, :]\n",
    "                )\n",
    "                pi_hats[eps].append(\n",
    "                    model_temp(X_s1[i]).squeeze()\n",
    "                )\n",
    "\n",
    "        # reshape stacked privatised estimated propensity scores as ne * nd\n",
    "        pi_hats[eps] = torch.stack(pi_hats[eps]).reshape(ne, nd, est_split)\n",
    "        \n",
    "        # max of abs of Y_s1 / propensity score for each experiment\n",
    "        max_abs_Y_s1_div_ps = torch.max(\n",
    "            torch.abs(Y1_s1 + Y0_s1) / pi_hats[eps], 2\n",
    "        )[0]\n",
    "        # max of abs of Y_s1 / (1 - propensity score) for each experiment\n",
    "        max_abs_Y_s1_div_1_m_ps = torch.max(\n",
    "            torch.abs(Y1_s1 + Y0_s1) / (1 - pi_hats[eps]), 2\n",
    "        )[0]\n",
    "        # hstack max_abs_Y_s1_div_ps and max_abs_Y_s1_div_1_m_ps\n",
    "        max_abs_all = torch.cat(\n",
    "            (max_abs_Y_s1_div_ps, max_abs_Y_s1_div_1_m_ps), \n",
    "            1,\n",
    "        )\n",
    "        \n",
    "        # replace inf/nan with 1e20 for stability\n",
    "        max_abs_all[torch.isfinite(max_abs_all) == 0] = 1e20\n",
    "            \n",
    "        # define sensitivity for estimation\n",
    "        s_e = 1 / (ns - nf) * 2 * torch.max(max_abs_all, 1)[0]\n",
    "        \n",
    "        # define sigma for estimation\n",
    "        sigma_e = np.sqrt(\n",
    "            2 * np.log(1.25 / delta) + 1e-10\n",
    "        ) * (s_e / eps)\n",
    "        sigma_e_2 = sigma_e ** 2\n",
    "        \n",
    "#         # analytic gaussian mechanism\n",
    "#         sigma_e = calibrateAnalyticGaussianMechanism(eps, delta, s_e)\n",
    "#         sigma_e_2 = sigma_e ** 2\n",
    "\n",
    "        # define e distribution for estimation\n",
    "        e_dist = torch.distributions.multivariate_normal.MultivariateNormal(\n",
    "            torch.tensor([0.0], dtype=torch.float64),\n",
    "            torch.diag(sigma_e)\n",
    "        )\n",
    "\n",
    "        # draw e for estimation\n",
    "        e[eps] = e_dist.sample().reshape(ne)\n",
    "    \n",
    "    # get treatment effects\n",
    "    # true\n",
    "    te = {}\n",
    "    # empirical means and std of means of ERM + private ERM\n",
    "    te_hats = {'means': [], 'stds': []}\n",
    "    # means and std of means of privatised te_hats\n",
    "    te_hats_p = {'means': [], 'stds': []}\n",
    "\n",
    "    # estimate true treatment effect\n",
    "    te_ = torch.mean(\n",
    "        Y1_s1.squeeze() / prob_vec[idx, s1]\n",
    "        - Y0_s1.squeeze() / (1 - prob_vec[idx, s1]),\n",
    "        1,\n",
    "    )\n",
    "    te['mean'] = te_.mean().detach().numpy()\n",
    "    te['std'] = te_.std().detach().numpy()\n",
    "                \n",
    "    for key in pi_hats.keys():\n",
    "        if key != 0:\n",
    "            # empirical estimates\n",
    "            # reduce_mean from (ne, nd, est_split) tensor to (ne * nd, 1) matrix\n",
    "            te_hats_ = torch.mean(\n",
    "                Y1_s1 / pi_hats[key] - Y0_s1 / (1 - pi_hats[key]), \n",
    "                [1, 2],\n",
    "            )\n",
    "        else:\n",
    "            # empirical estimate for noiseless case\n",
    "            # reduce_mean from (ne, est_split) tensor to (ne , 1) matrix\n",
    "            te_hats_ = torch.mean(\n",
    "                Y1_s1.squeeze() / pi_hats[key] - Y0_s1.squeeze() / (1 - pi_hats[key]),\n",
    "                1,\n",
    "            )\n",
    "        te_hats['means'].append(\n",
    "            te_hats_.detach().numpy()\n",
    "        )\n",
    "        te_hats['stds'].append(\n",
    "            te_hats_.std().detach().numpy()\n",
    "        )\n",
    "        try:\n",
    "            te_hats_p_ = te_hats_ + e[key]\n",
    "            te_hats_p['means'].append(\n",
    "                te_hats_p_.detach().numpy()\n",
    "            )\n",
    "            te_hats_p['stds'].append(\n",
    "                te_hats_p_.std().detach().numpy()\n",
    "            )\n",
    "        except KeyError:\n",
    "            # fill first row for later\n",
    "            te_hats_p['means'].append(\n",
    "                te_hats_.detach().numpy()\n",
    "            )\n",
    "            te_hats_p['stds'].append(\n",
    "                te_hats_.std().detach().numpy()\n",
    "            )\n",
    "        \n",
    "    te_hats['means'] = np.array(te_hats['means'])\n",
    "    te_hats['stds'] = np.array(te_hats['stds'])\n",
    "    te_hats_p['means'] = np.array(te_hats_p['means'])\n",
    "    te_hats_p['stds'] = np.array(te_hats_p['stds'])\n",
    "\n",
    "    return te, te_hats, te_hats_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mult_te(figname, te_list, te_hats_list_, ns, nf, epses, tau):\n",
    "    '''\n",
    "    plot the true treatment effect, ERM, private ERM treatment effect\n",
    "    '''        \n",
    "\n",
    "    # process inputs\n",
    "    te_hat_list, te_hats_z_list, te_hats_mu_list = {'mu':[], 'std':[]}, {'mu':[], 'std':[]}, {'mu':[], 'std':[]}\n",
    "\n",
    "    for i in range(len(nf)):\n",
    "        te_hat_list['mu'].append(np.mean([te_hats_list_[i]['means'][0]], 1))\n",
    "        te_hat_list['std'].append(np.std([te_hats_list_[i]['means'][0]]))\n",
    "        te_hats_z_list['mu'].append(np.mean(te_hats_list_[i]['means'][1:], 1))\n",
    "        te_hats_z_list['std'].append(np.std(te_hats_list_[i]['means'][1:], 1))\n",
    "\n",
    "    # calculate number of points used to estimate tau\n",
    "    n_est = ns[0] - nf[0]\n",
    "        \n",
    "  # plot figure\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "\n",
    "    ax.plot(\n",
    "        nf,\n",
    "        [i['mean'] for i in te_list],\n",
    "        marker='o',\n",
    "        color='magenta',\n",
    "        lw=3,\n",
    "        label=\"True $\\\\hat{\\\\tau}$\",\n",
    "    )\n",
    "    ax.errorbar(\n",
    "        nf,\n",
    "        np.array(\n",
    "            [te_hat_list['mu'][i] for i in range(len(nf))]\n",
    "        ).squeeze(),\n",
    "        1.96\n",
    "        / np.sqrt(n_est)\n",
    "        * np.array(\n",
    "            [te_hat_list['std'][i] for i in range(len(nf))]\n",
    "        ).squeeze(),\n",
    "        marker='o',\n",
    "        color='red',\n",
    "        lw=3,\n",
    "        label=\"$\\\\hat{\\\\tau}$\",\n",
    "    )\n",
    "    # initialise colour map\n",
    "    col_map = plt.cm.get_cmap('Set1')\n",
    "\n",
    "    for i in range(1, len(epses) + 1):\n",
    "        # get color\n",
    "        col = col_map(i / (len(epses) + 1))\n",
    "        te_hats_z_mu_ = np.array(\n",
    "            [\n",
    "                te_hats_z_list['mu'][j][i-1]\n",
    "                for j in range(len(nf))\n",
    "            ]\n",
    "        ).squeeze()\n",
    "        te_hats_z_std_ = np.array(\n",
    "            [\n",
    "                te_hats_z_list['std'][j][i-1]\n",
    "                for j in range(len(nf))\n",
    "            ]\n",
    "        ).squeeze()\n",
    "        ax.errorbar(\n",
    "            nf,\n",
    "            te_hats_z_mu_,\n",
    "            1.96 / np.sqrt(n_est) * te_hats_z_std_,\n",
    "            marker='o',\n",
    "            color=col,\n",
    "            lw=3,\n",
    "            label=\"$\\\\hat{\\\\tau}_{\\\\epsilon}$ for $\\epsilon$=\"\n",
    "            + str(epses[i-1]),\n",
    "        )\n",
    "\n",
    "    ax.set_title(\n",
    "        \"True $\\\\hat{\\\\tau}$\"\n",
    "        + \", $\\\\hat{\\\\tau}$\"\n",
    "        + \", $\\\\hat{\\\\tau}_{\\\\epsilon}$\"\n",
    "        + \" against $m$ for $\\\\tau$ = {}\".format(tau),\n",
    "        fontsize=20,\n",
    "    )\n",
    "    ax.set_xlabel(\"sample size ($m$)\", fontsize=18)\n",
    "    ax.set_xticks(nf)\n",
    "    ax.set_xticklabels(\n",
    "        [\n",
    "            \"$0.5\\\\times10^3$\",\n",
    "            \"$10^3$\",\n",
    "            \"$1.5\\\\times10^3$\",\n",
    "            \"$2\\\\times10^3$\",\n",
    "            \"$2.5\\\\times10^3$\",\n",
    "#             \"$3\\\\times10^3$\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    ax.tick_params(labelsize=16)\n",
    "\n",
    "    if tau > 0.1:\n",
    "        ax.legend(fontsize=16, loc=1)\n",
    "    else:\n",
    "        ax.legend(fontsize=16, loc=4)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(figname + '.pdf', dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\tau = 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lists for results\n",
    "te_list, te_hats_list_, te_hats_p_list_ = [], [], []\n",
    "\n",
    "for i in range(len(ns)):\n",
    "    # draw ne separate ns samples\n",
    "    X_std = 3\n",
    "    X_dist = torch.distributions.normal.Normal(\n",
    "        torch.tensor([0.0], dtype=torch.float64), \n",
    "        torch.tensor([X_std], dtype=torch.float64)\n",
    "    )\n",
    "    X = [X_dist.sample((ns[i], dim)).squeeze() for j in range(ne)]\n",
    "    \n",
    "    # restrict X to ||x||_2 \\leq 1 to fit assumption for each experiment\n",
    "    X = torch.stack([X[j] / X[j].norm(dim=1).max() for j in range(ne)])\n",
    "\n",
    "    # generate Y\n",
    "    Y_std = 0.1\n",
    "    Y = torch.einsum('kl,ijk->ij',beta, X) + Y_std * torch.randn(ne, ns[i], dtype=torch.float64)\n",
    "    \n",
    "    # generate T \\in {0, 1}\n",
    "    prob_vec = torch.sigmoid(torch.einsum('kl,ijk->ij', T_w, X) + T_b)\n",
    "    T = torch.bernoulli(prob_vec)\n",
    "\n",
    "    te, te_hats, te_hats_p = IPTW_PPS(X, T, prob_vec, Y, tau, epses, delta, reg_co, nd, nf[i])\n",
    "    \n",
    "    te_list.append(te); te_hats_list_.append(te_hats); te_hats_p_list_.append(te_hats_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_mult_te('sim_tau_'+str(tau)+'_m', te_list, te_hats_p_list_, ns, nf, epses, tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
