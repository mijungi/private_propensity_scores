{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of $\\epsilon$ on Average Treatment Effect on IHDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1276dbe90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from misc.agm import calibrateAnalyticGaussianMechanism\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# set random seed\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no. samples for fitting, no. samples for estimating, no. of draws of z\n",
    "nf = 500\n",
    "nt = 500\n",
    "nd = 1\n",
    "\n",
    "# privacy parameters\n",
    "epses = [0, 0.2, 0.4, 0.6, 0.8, 0.99]\n",
    "delta = torch.tensor(1e-6)\n",
    "\n",
    "# regularisation coefficient\n",
    "reg_co = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess IHDP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "train = {}\n",
    "\n",
    "with np.load('data/IHDP-1000/ihdp_npci_1-1000.train.npz') as data:\n",
    "    for i in data.files:\n",
    "        # convert all arrays to torch tensors\n",
    "        train[i] = torch.tensor(data[i], dtype=torch.float64)\n",
    "        \n",
    "# ate is the true average treatment effect\n",
    "# yf is the noisy factual outcome\n",
    "# need only, ate, yf, t and x\n",
    "\n",
    "# swap axis and preprocess x\n",
    "for i in  ['yf', 't', 'x']:\n",
    "    if i != 'x':\n",
    "        train[i] = train[i].transpose(0, 1)\n",
    "    else:\n",
    "        train[i] = train[i].permute(2, 0, 1)\n",
    "        \n",
    "# get no. experiments and dim\n",
    "ne, _, d = train['x'].shape\n",
    "\n",
    "# change ne if required\n",
    "# ne = 250\n",
    "\n",
    "# generate X, T, Y through subsampling\n",
    "Y_train, X_train = [], []\n",
    "T_train = torch.stack(\n",
    "    [torch.cat([torch.ones(int(nf/2), dtype=torch.float64), torch.zeros(int(nf/2), dtype=torch.float64)])] * ne\n",
    ")\n",
    "\n",
    "for i in range(ne):\n",
    "    # get indices for t=1 and t=0\n",
    "    t1_idx = train['t'][i, :].nonzero().squeeze()\n",
    "    t0_idx = (1 - train['t'][i, :]).nonzero().squeeze()\n",
    "                 \n",
    "    # subsample n indices, n/2 for t=1 and n/2 for t=0\n",
    "    sam_idx = np.hstack([np.random.choice(t1_idx, int(nf/2)), np.random.choice(t0_idx, int(nf/2))])\n",
    "    \n",
    "    Y_train.append(train['yf'][i, sam_idx])\n",
    "    X_train.append(train['x'][i, sam_idx, :])\n",
    "    \n",
    "# convert to torch tensors \n",
    "Y_train, X_train = torch.stack(Y_train), torch.stack(X_train)\n",
    "\n",
    "# permute data\n",
    "# permute indices\n",
    "perm = torch.stack([torch.randperm(nf) for i in range(ne)])\n",
    "\n",
    "# create auxiliary indices\n",
    "idx = torch.arange(ne)[:, None]\n",
    "\n",
    "# permute X_train, T_train, Y_train\n",
    "X_train = X_train[idx, perm]\n",
    "T_train = T_train[idx, perm]\n",
    "Y_train = Y_train[idx, perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data\n",
    "test = {}\n",
    "\n",
    "with np.load('data/IHDP-1000/ihdp_npci_1-1000.test.npz') as data:\n",
    "    for i in data.files:\n",
    "        # convert all arrays to torch tensors\n",
    "        test[i] = torch.tensor(data[i], dtype=torch.float64)\n",
    "        \n",
    "# ate is the true average treatment effect\n",
    "# yf is the noisy factual outcome\n",
    "# need only, ate, yf, t and x\n",
    "\n",
    "# swap axis and preprocess x\n",
    "for i in  ['yf', 't', 'x']:\n",
    "    if i != 'x':\n",
    "        test[i] = test[i].transpose(0, 1)\n",
    "    else:\n",
    "        test[i] = test[i].permute(2, 0, 1)\n",
    "        \n",
    "# get no. experiments and dim\n",
    "ne, _, d = test['x'].shape\n",
    "\n",
    "# change ne if required\n",
    "# ne = 250\n",
    "\n",
    "# generate X_test, T_test, Y_test through subsampling\n",
    "Y_test, X_test = [], []\n",
    "T_test = torch.stack([torch.cat(\n",
    "    [torch.ones(int(nt/2), dtype=torch.float64), torch.zeros(int(nt/2), dtype=torch.float64)])] * ne\n",
    ")\n",
    "\n",
    "for i in range(ne):\n",
    "    # get indices for t=1 and t=0\n",
    "    t1_idx = test['t'][i, :].nonzero().squeeze()\n",
    "    t0_idx = (1 - test['t'][i, :]).nonzero().squeeze()\n",
    "                 \n",
    "    # subsample n indices, n/2 for t=1 and n/2 for t=0\n",
    "    sam_idx = np.hstack([np.random.choice(t1_idx, int(nt/2)), np.random.choice(t0_idx, int(nt/2))])\n",
    "    \n",
    "    Y_test.append(test['yf'][i, sam_idx])\n",
    "    X_test.append(test['x'][i, sam_idx, :])\n",
    "    \n",
    "# convert to torch tensors \n",
    "Y_test, X_test = torch.stack(Y_test), torch.stack(X_test)\n",
    "\n",
    "# permute data\n",
    "# permute indices\n",
    "perm = torch.stack([torch.randperm(nt) for i in range(ne)])\n",
    "\n",
    "# create auxiliary indices\n",
    "idx = torch.arange(ne)[:, None]\n",
    "\n",
    "# permute X_train, T_train, Y_train\n",
    "X_test = X_test[idx, perm]\n",
    "T_test = T_test[idx, perm]\n",
    "Y_test = Y_test[idx, perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate train and test\n",
    "X_all = torch.cat([X_train, X_test], 1)\n",
    "T_all = torch.cat([T_train, T_test], 1)\n",
    "Y_all = torch.cat([Y_train, Y_test], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model and method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Log_Reg(torch.nn.Module):\n",
    "    '''\n",
    "    Logistic Regression\n",
    "    '''\n",
    "    def __init__(self, D_in, D_out):\n",
    "        super(Log_Reg, self).__init__()\n",
    "        self.linear = torch.nn.Linear(D_in, D_out, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = torch.sigmoid(self.linear(x))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPW_PPS_Out(X, T, Y, epses, delta, reg_co, nf):\n",
    "    '''\n",
    "    average treatment effect with inverse propensity weighting using private propensity scores\n",
    "    '''\n",
    "    # get # experiments, # samples, # dimensions\n",
    "    ne, ns, dim = X.shape\n",
    "\n",
    "    ################\n",
    "    # process data #\n",
    "    ################\n",
    "\n",
    "    # get Y0 and Y1\n",
    "    Y0 = Y * (1 - T)\n",
    "    Y1 = Y * T\n",
    "    \n",
    "    # split data\n",
    "    # get splits\n",
    "    fit_split = nf\n",
    "    est_split = ns - nf\n",
    "\n",
    "    # create auxiliary indices\n",
    "    idx = torch.arange(ne)[:, None]\n",
    "\n",
    "    # split X into fit, estimate splits\n",
    "    X_s0 = X[:, :fit_split]\n",
    "    X_s1 = X[:, fit_split:]\n",
    "\n",
    "    # expand dim of T to allow multiplication with X\n",
    "    T_ex_dim = T.reshape(ne, ns, 1)\n",
    "\n",
    "    # split X0 and X1 into fit, estimate splits\n",
    "    X0_s1 = (X * (1 - T_ex_dim))[:, fit_split:]\n",
    "    X1_s1 = (X * T_ex_dim)[:, fit_split:]\n",
    "\n",
    "    # split T into fit, estimate splits\n",
    "    T_s0 = T[:, :fit_split]\n",
    "    T_s1 = T[:, fit_split:]\n",
    "        \n",
    "    # split Y0 and Y1 into fit, estimate splits\n",
    "    Y0_s0 = Y0[:, :fit_split]\n",
    "    Y1_s0 = Y1[:, :fit_split]\n",
    "\n",
    "    Y0_s1 = Y0[:, fit_split:]\n",
    "    Y1_s1 = Y1[:, fit_split:]\n",
    "    \n",
    "    ##############\n",
    "    # fit models #\n",
    "    ##############\n",
    "    \n",
    "    models = []\n",
    "    \n",
    "    for expm in range(ne):\n",
    "        X = X_s0[expm]\n",
    "        T = T_s0[expm][:, None]\n",
    "        model = Log_Reg(dim, 1).double()\n",
    "        opt = torch.optim.LBFGS(model.parameters(), max_iter=100)\n",
    "\n",
    "        # define first-order oracle for lbfgs\n",
    "        def closure():\n",
    "            if torch.is_grad_enabled():\n",
    "                opt.zero_grad()\n",
    "            outputs = model(X)\n",
    "            for weights in model.parameters():\n",
    "                loss = torch.nn.functional.binary_cross_entropy_with_logits(outputs, T) + 0.5 * reg_co * weights.norm(2).pow(2)\n",
    "            if loss.requires_grad:\n",
    "                loss.backward()\n",
    "            return loss\n",
    "\n",
    "        opt.step(closure)\n",
    "\n",
    "        models.append(model)\n",
    "\n",
    "    #############################\n",
    "    # estimate treatment effect #\n",
    "    #############################\n",
    "\n",
    "    # initialise pi_hat dictionaries\n",
    "    pi_hats = {}\n",
    "    \n",
    "    # initialise e dictionary\n",
    "    e = {}\n",
    "    \n",
    "    # intialise sigma dictionary\n",
    "    sig_d = {}\n",
    "\n",
    "    # get estimated propensity scores\n",
    "    pi_hats[0] = torch.stack(\n",
    "        [models[i](X_s1[i]).squeeze() for i in range(ne)]\n",
    "    )\n",
    "\n",
    "    # perturb model and get relevant quantities\n",
    "    for eps in epses[1:]:\n",
    "        # define sensitivity for log reg\n",
    "        s_w = 2.0 / (fit_split * reg_co)\n",
    "\n",
    "        # define sigma for log reg\n",
    "        sigma = np.sqrt(\n",
    "            2 * np.log(1.25 / delta) + 1e-10\n",
    "        ) * (s_w / (eps / 2))\n",
    "        sigma_2 = sigma ** 2\n",
    "\n",
    "#         # analytic gaussian mechanism\n",
    "#         sigma = calibrateAnalyticGaussianMechanism(eps, delta, s_w)\n",
    "#         sigma_2 = sigma ** 2\n",
    "\n",
    "        # define z distribution for log reg\n",
    "        z_dist = torch.distributions.normal.Normal(\n",
    "            torch.tensor(0.0, dtype=torch.float64),\n",
    "            torch.tensor(sigma, dtype=torch.float64),\n",
    "        )\n",
    "\n",
    "        # draw z for log reg\n",
    "        z_vecs = z_dist.sample((ne, dim))\n",
    "\n",
    "        # create temp models\n",
    "        models_ = copy.deepcopy(models)\n",
    "\n",
    "        # initialise list for privatised estimated propensity scores\n",
    "        pi_hats[eps] = []\n",
    "\n",
    "        # perturb weights with z_vecs\n",
    "        for i in range(ne):\n",
    "            model_temp = models_[i]\n",
    "            model_temp.linear.weight.data.add_(\n",
    "                z_vecs[i]\n",
    "            )\n",
    "            pi_hats[eps].append(\n",
    "                model_temp(X_s1[i]).squeeze()\n",
    "            )\n",
    "\n",
    "        # reshape stacked privatised estimated propensity scores\n",
    "        pi_hats[eps] = torch.stack(pi_hats[eps])\n",
    "                        \n",
    "        # max of abs of Y1_s1 / propensity score for each experiment\n",
    "        max_abs_Y1_s1_div_ps = torch.max(\n",
    "            torch.abs(Y1_s1) / ((ns - nf) * pi_hats[eps]), 1\n",
    "        )[0]\n",
    "        \n",
    "        # max of abs of Y0_s1 / (1 - propensity score) for each experiment\n",
    "        max_abs_Y0_s1_div_1_m_ps = torch.max(\n",
    "            torch.abs(Y0_s1) / ((ns - nf) * (1 - pi_hats[eps])), 1\n",
    "        )[0]\n",
    "        \n",
    "        # hstack max_abs_Y_s1_div_ps and max_abs_Y_s1_div_1_m_ps\n",
    "        max_abs_all = torch.stack(\n",
    "            (max_abs_Y1_s1_div_ps, max_abs_Y0_s1_div_1_m_ps), 1\n",
    "        )\n",
    "        \n",
    "        # replace inf/nan with 1e20 for stability\n",
    "        max_abs_all[torch.isfinite(max_abs_all) == 0] = 1e20\n",
    "            \n",
    "        # define sensitivity for estimation\n",
    "        s_e = 2 * torch.max(max_abs_all, 1)[0]\n",
    "        \n",
    "        # define sigma for estimation\n",
    "        sigma_e = np.sqrt(\n",
    "            2 * np.log(1.25 / delta) + 1e-10\n",
    "        ) * (s_e / (eps / 2))\n",
    "        sig_d[eps] = sigma_e.detach().numpy()\n",
    "        sigma_e_2 = sigma_e ** 2\n",
    "        \n",
    "#         # analytic gaussian mechanism\n",
    "#         sigma_e = calibrateAnalyticGaussianMechanism(eps, delta, s_e)\n",
    "#         sigma_e_2 = sigma_e ** 2\n",
    "\n",
    "        # define e distribution for estimation\n",
    "        e_dist = torch.distributions.multivariate_normal.MultivariateNormal(\n",
    "            torch.tensor([0.0], dtype=torch.float64),\n",
    "            torch.diag(sigma_e)\n",
    "        )\n",
    "\n",
    "        # draw e for estimation\n",
    "        e[eps] = e_dist.sample().reshape(ne)\n",
    "    \n",
    "    # get treatment effects\n",
    "    # true\n",
    "    te = {}\n",
    "    # empirical means and std of means of ERM + private ERM\n",
    "    te_hats = {'means': [], 'stds': []}\n",
    "    # means and std of means of privatised te_hats\n",
    "    te_hats_p = {'means': [], 'stds': []}\n",
    "                \n",
    "    for key in pi_hats.keys():\n",
    "        # empirical estimate for noiseless case\n",
    "        # reduce_mean from (ne, est_split) tensor to (ne , 1) matrix\n",
    "        te_hats_ = torch.mean(\n",
    "            Y1_s1 / pi_hats[key] - Y0_s1 / (1 - pi_hats[key]),\n",
    "            1,\n",
    "        )\n",
    "        te_hats['means'].append(\n",
    "            te_hats_.detach().numpy()\n",
    "        )\n",
    "        te_hats['stds'].append(\n",
    "            te_hats_.std().detach().numpy()\n",
    "        )\n",
    "        try:\n",
    "            te_hats_p_ = te_hats_ + e[key]\n",
    "            te_hats_p['means'].append(\n",
    "                te_hats_p_.detach().numpy()\n",
    "            )\n",
    "            te_hats_p['stds'].append(\n",
    "                te_hats_p_.std().detach().numpy()\n",
    "            )\n",
    "        except KeyError:\n",
    "            # fill first row for later\n",
    "            te_hats_p['means'].append(\n",
    "                te_hats_.detach().numpy()\n",
    "            )\n",
    "            te_hats_p['stds'].append(\n",
    "                te_hats_.std().detach().numpy()\n",
    "            )\n",
    "        \n",
    "    te_hats['means'] = np.array(te_hats['means'])\n",
    "    te_hats['stds'] = np.array(te_hats['stds'])\n",
    "    te_hats_p['means'] = np.array(te_hats_p['means'])\n",
    "    te_hats_p['stds'] = np.array(te_hats_p['stds'])\n",
    "\n",
    "    return te, te_hats, te_hats_p, sig_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPW_PPS_Obj(X, T, Y, epses, delta, reg_co, nf):\n",
    "    '''\n",
    "    average treatment effect with inverse propensity weighting using private propensity scores\n",
    "    '''\n",
    "    # get # experiments, # samples, # dimensions\n",
    "    ne, ns, dim = X.shape\n",
    "\n",
    "    # objective perturbation constants\n",
    "    L = 1 # see from derivation, also http://proceedings.mlr.press/v32/jain14.pdf\n",
    "    R2 = 1 # as norm is bounded by 1\n",
    "    c = 0.25\n",
    "    \n",
    "    ################\n",
    "    # process data #\n",
    "    ################\n",
    "\n",
    "    # get Y0 and Y1\n",
    "    Y0 = Y * (1 - T)\n",
    "    Y1 = Y * T\n",
    "    \n",
    "    # split data\n",
    "    # get splits\n",
    "    fit_split = nf\n",
    "    est_split = ns - nf\n",
    "\n",
    "    # create auxiliary indices\n",
    "    idx = torch.arange(ne)[:, None]\n",
    "\n",
    "    # split X into fit, estimate splits\n",
    "    X_s0 = X[:, :fit_split]\n",
    "    X_s1 = X[:, fit_split:]\n",
    "\n",
    "    # expand dim of T to allow multiplication with X\n",
    "    T_ex_dim = T.reshape(ne, ns, 1)\n",
    "\n",
    "    # split X0 and X1 into fit, estimate splits\n",
    "    X0_s1 = (X * (1 - T_ex_dim))[:, fit_split:]\n",
    "    X1_s1 = (X * T_ex_dim)[:, fit_split:]\n",
    "\n",
    "    # split T into fit, estimate splits\n",
    "    T_s0 = T[:, :fit_split]\n",
    "    T_s1 = T[:, fit_split:]\n",
    "        \n",
    "    # split Y0 and Y1 into fit, estimate splits\n",
    "    Y0_s0 = Y0[:, :fit_split]\n",
    "    Y1_s0 = Y1[:, :fit_split]\n",
    "\n",
    "    Y0_s1 = Y0[:, fit_split:]\n",
    "    Y1_s1 = Y1[:, fit_split:]\n",
    "    \n",
    "    ##############\n",
    "    # fit models #\n",
    "    ##############\n",
    "\n",
    "    z_dist = torch.distributions.normal.Normal(\n",
    "                torch.tensor(0.0, dtype=torch.double),\n",
    "                torch.tensor(1.0, dtype=torch.double),\n",
    "                )\n",
    "    \n",
    "    models = {}\n",
    "    \n",
    "    for eps in epses:\n",
    "        models[eps] = []\n",
    "        for expm in range(ne):\n",
    "            X = X_s0[expm]\n",
    "            T = T_s0[expm][:, None]\n",
    "            model = Log_Reg(dim, 1).double()\n",
    "            opt = torch.optim.LBFGS(model.parameters(), max_iter=100)\n",
    "            if eps > 0: \n",
    "                b = torch.sqrt((4 * (L * R2) ** 2 * (torch.log(1 / delta) + eps / 2)) / ((eps / 2) ** 2)) * z_dist.sample((dim, 1))\n",
    "                # b = torch.sqrt((8 * (torch.log(2. / delta) + 4 * eps)) / (eps ** 2)) * z_dist.sample((dim, 1))\n",
    "            else:\n",
    "                b = torch.zeros((dim, 1)).double()\n",
    "            \n",
    "            # define first-order oracle for lbfgs\n",
    "            def closure():\n",
    "                if torch.is_grad_enabled():\n",
    "                    opt.zero_grad()\n",
    "                outputs = model(X)\n",
    "                if eps > 0:\n",
    "                    for weights in model.parameters():\n",
    "                        reg_noise = 1 / nf * torch.matmul(weights, b) + 0.5 * reg_co * weights.norm(2).pow(2)\n",
    "                        # reg_noise = 1 / nf * torch.matmul(weights, b) + 0.5 * (2 * c * X_std / (eps * nf) + reg_co) * weights.norm(2).pow(2)\n",
    "                else:\n",
    "                    for weights in model.parameters():\n",
    "                        reg_noise = 0.5 * reg_co * weights.norm(2).pow(2)\n",
    "                loss = torch.nn.functional.binary_cross_entropy_with_logits(outputs, T) + reg_noise\n",
    "                if loss.requires_grad:\n",
    "                    loss.backward()\n",
    "                return loss\n",
    "            \n",
    "            opt.step(closure)\n",
    "\n",
    "            models[eps].append(model)\n",
    "      \n",
    "    #############################\n",
    "    # estimate treatment effect #\n",
    "    #############################\n",
    "\n",
    "    # initialise pi_hat dictionaries\n",
    "    pi_hats = {}\n",
    "    \n",
    "    # initialise e dictionary\n",
    "    e = {}\n",
    "    \n",
    "    # intialise sigma dictionary\n",
    "    sig_d = {}\n",
    "\n",
    "    # get estimated propensity scores\n",
    "    pi_hats[0] = torch.stack(\n",
    "        [models[0][i](X_s1[i]).squeeze() for i in range(ne)]\n",
    "    )\n",
    "\n",
    "    for eps in epses[1:]:\n",
    "        # get perturbed propensity scores\n",
    "        pi_hats[eps] = torch.stack(\n",
    "            [models[eps][i](X_s1[i]).squeeze() for i in range(ne)]\n",
    "        )\n",
    "                \n",
    "        # max of abs of Y1_s1 / propensity score for each experiment\n",
    "        max_abs_Y1_s1_div_ps = torch.max(\n",
    "            torch.abs(Y1_s1) / ((ns - nf) * pi_hats[eps]), 1\n",
    "        )[0]\n",
    "                \n",
    "        # max of abs of Y0_s1 / (1 - propensity score) for each experiment\n",
    "        max_abs_Y0_s1_div_1_m_ps = torch.max(\n",
    "            torch.abs(Y0_s1) / ((ns - nf) * (1 - pi_hats[eps])), 1\n",
    "        )[0]\n",
    "        \n",
    "        # hstack max_abs_Y_s1_div_ps and max_abs_Y_s1_div_1_m_ps\n",
    "        max_abs_all = torch.stack(\n",
    "            (max_abs_Y1_s1_div_ps, max_abs_Y0_s1_div_1_m_ps), 1\n",
    "        )\n",
    "                \n",
    "        # replace inf/nan with 1e20 for stability\n",
    "        max_abs_all[torch.isfinite(max_abs_all) == 0] = 1e20\n",
    "            \n",
    "        # define sensitivity for estimation\n",
    "        s_e = 2 * torch.max(max_abs_all, 1)[0]\n",
    "        \n",
    "        # define sigma for estimation\n",
    "        sigma_e = np.sqrt(\n",
    "            2 * np.log(1.25 / delta) + 1e-10\n",
    "        ) * (s_e / (eps / 2))\n",
    "        sig_d[eps] = sigma_e.detach().numpy()\n",
    "        sigma_e_2 = sigma_e ** 2\n",
    "        \n",
    "#         # analytic gaussian mechanism\n",
    "#         sigma_e = calibrateAnalyticGaussianMechanism(eps, delta, s_e)\n",
    "#         sigma_e_2 = sigma_e ** 2\n",
    "\n",
    "        # define e distribution for estimation\n",
    "        e_dist = torch.distributions.multivariate_normal.MultivariateNormal(\n",
    "            torch.tensor([0.0], dtype=torch.float64),\n",
    "            torch.diag(sigma_e)\n",
    "        )\n",
    "\n",
    "        # draw e for estimation\n",
    "        e[eps] = e_dist.sample().reshape(ne)\n",
    "    \n",
    "    # get treatment effects\n",
    "    # true\n",
    "    te = {}\n",
    "    # empirical means and std of means of ERM + private ERM\n",
    "    te_hats = {'means': [], 'stds': []}\n",
    "    # means and std of means of privatised te_hats\n",
    "    te_hats_p = {'means': [], 'stds': []}\n",
    "             \n",
    "    for key in pi_hats.keys():\n",
    "        # reduce_mean from (ne, est_split) tensor to (ne, 1) matrix\n",
    "        te_hats_ = torch.mean(\n",
    "            Y1_s1 / pi_hats[key] - Y0_s1 / (1 - pi_hats[key]), \n",
    "            1,\n",
    "        )\n",
    "        te_hats['means'].append(\n",
    "            te_hats_.detach().numpy()\n",
    "        )\n",
    "        te_hats['stds'].append(\n",
    "            te_hats_.std().detach().numpy()\n",
    "        )\n",
    "        try:\n",
    "            te_hats_p_ = te_hats_ + e[key]\n",
    "            te_hats_p['means'].append(\n",
    "                te_hats_p_.detach().numpy()\n",
    "            )\n",
    "            te_hats_p['stds'].append(\n",
    "                te_hats_p_.std().detach().numpy()\n",
    "            )\n",
    "        except KeyError:\n",
    "            # fill first row for later\n",
    "            te_hats_p['means'].append(\n",
    "                te_hats_.detach().numpy()\n",
    "            )\n",
    "            te_hats_p['stds'].append(\n",
    "                te_hats_.std().detach().numpy()\n",
    "            )\n",
    "        \n",
    "    te_hats['means'] = np.array(te_hats['means'])\n",
    "    te_hats['stds'] = np.array(te_hats['stds'])\n",
    "    te_hats_p['means'] = np.array(te_hats_p['means'])\n",
    "    te_hats_p['stds'] = np.array(te_hats_p['stds'])\n",
    "\n",
    "    return te, te_hats, te_hats_p, sig_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run method and print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sikai/.pyenv/versions/3.7.6/lib/python3.7/site-packages/ipykernel_launcher.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "te, te_hats, te_hats_p, sig_d = IPW_PPS_Out(X_all, T_all, Y_all, epses, delta, reg_co, nf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean ATE for no epsilon is 15.033430336156096 and its s.e. is 0.35819745636166966\n",
      "The non-privatised mean ATE for epsilon = 0 is -inf and its s.e. is nan\n",
      "The non-privatised mean ATE for epsilon = 0.2 is 563267.6525562855 and its s.e. is 613395.4503160334\n",
      "The non-privatised mean ATE for epsilon = 0.4 is 817.7294752865399 and its s.e. is 314.4303943779342\n",
      "The non-privatised mean ATE for epsilon = 0.6 is 67.20964227857449 and its s.e. is 24.674317498045625\n",
      "The non-privatised mean ATE for epsilon = 0.8 is 54.31971501471241 and its s.e. is 7.455008276797289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sikai/.pyenv/versions/3.7.6/lib/python3.7/site-packages/numpy/core/_methods.py:202: RuntimeWarning: invalid value encountered in subtract\n",
      "  x = asanyarray(arr - arrmean)\n"
     ]
    }
   ],
   "source": [
    "means = [np.mean(i) for i in te_hats['means']]\n",
    "se = [np.std(i) / np.sqrt(ne) for i in te_hats['means']]\n",
    "\n",
    "for i in range(len(means)):\n",
    "    if i == 0:\n",
    "        print('The mean ATE for no epsilon is {} and its s.e. is {}'.format(means[i], se[i]))\n",
    "    else:\n",
    "        print('The non-privatised mean ATE for epsilon = {} is {} and its s.e. is {}'.format(epses[i-1], means[i], se[i]))                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean ATE for no epsilon is 15.033430336156096 and its s.e. is 0.35819745636166966\n",
      "The privatised mean ATE for epsilon = 0 is -inf and its s.e. is nan\n",
      "The privatised mean ATE for epsilon = 0.2 is 563243.0776508197 and its s.e. is 613376.969479873\n",
      "The privatised mean ATE for epsilon = 0.4 is 817.8445000575667 and its s.e. is 315.32626946345334\n",
      "The privatised mean ATE for epsilon = 0.6 is 66.50325492294587 and its s.e. is 24.698620124944828\n",
      "The privatised mean ATE for epsilon = 0.8 is 54.32102945682354 and its s.e. is 7.501413817779162\n"
     ]
    }
   ],
   "source": [
    "means = [np.mean(i) for i in te_hats_p['means']]\n",
    "se = [np.std(i) / np.sqrt(ne) for i in te_hats_p['means']]\n",
    "\n",
    "for i in range(len(means)):\n",
    "    if i == 0:\n",
    "        print('The mean ATE for no epsilon is {} and its s.e. is {}'.format(means[i], se[i]))\n",
    "    else:\n",
    "        print('The privatised mean ATE for epsilon = {} is {} and its s.e. is {}'.format(epses[i-1], means[i], se[i]))                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of signs being flipped for non-privatised ATE for epsilon = 0.2 is 0.528\n",
      "The probability of signs being flipped for non-privatised ATE for epsilon = 0.4 is 0.41\n",
      "The probability of signs being flipped for non-privatised ATE for epsilon = 0.6 is 0.314\n",
      "The probability of signs being flipped for non-privatised ATE for epsilon = 0.8 is 0.231\n",
      "The probability of signs being flipped for non-privatised ATE for epsilon = 0.99 is 0.176\n"
     ]
    }
   ],
   "source": [
    "sgn_tau_hat = np.sign(te_hats['means'][0])\n",
    "    \n",
    "# compute probabilities\n",
    "probs = [sum(np.sign(i) != sgn_tau_hat) / ne for i in te_hats['means'][1:]]\n",
    "\n",
    "for i in range(1, len(epses)):\n",
    "    print('The probability of signs being flipped for non-privatised ATE for epsilon = {} is {}'.format(epses[i], probs[i-1]))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of signs being flipped for privatised ATE for epsilon = 0.2 is 0.529\n",
      "The probability of signs being flipped for privatised ATE for epsilon = 0.4 is 0.413\n",
      "The probability of signs being flipped for privatised ATE for epsilon = 0.6 is 0.303\n",
      "The probability of signs being flipped for privatised ATE for epsilon = 0.8 is 0.23\n",
      "The probability of signs being flipped for privatised ATE for epsilon = 0.99 is 0.181\n"
     ]
    }
   ],
   "source": [
    "sgn_tau_hat = np.sign(te_hats_p['means'][0])\n",
    "    \n",
    "# compute probabilities\n",
    "probs = [sum(np.sign(i) != sgn_tau_hat) / ne for i in te_hats_p['means'][1:]]\n",
    "\n",
    "for i in range(1, len(epses)):\n",
    "    print('The probability of signs being flipped for privatised ATE for epsilon = {} is {}'.format(epses[i], probs[i-1]))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "te, te_hats, te_hats_p, sig_d = IPW_PPS_Obj(X_all, T_all, Y_all, epses, delta, reg_co, nf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean ATE for no epsilon is 15.03343853707586 and its s.e. is 0.35819117126040745\n",
      "The non-privatised mean ATE for epsilon = 0 is -3873168066.3329234 and its s.e. is 3820312742.8553967\n",
      "The non-privatised mean ATE for epsilon = 0.2 is 444.3367602418289 and its s.e. is 199.1621070492235\n",
      "The non-privatised mean ATE for epsilon = 0.4 is 67.13478969965367 and its s.e. is 10.01034770468249\n",
      "The non-privatised mean ATE for epsilon = 0.6 is 28.00095553344574 and its s.e. is 1.4673441398598515\n",
      "The non-privatised mean ATE for epsilon = 0.8 is 22.870310416137585 and its s.e. is 0.9651547074460028\n"
     ]
    }
   ],
   "source": [
    "means = [np.mean(i) for i in te_hats['means']]\n",
    "se = [np.std(i) / np.sqrt(ne) for i in te_hats['means']]\n",
    "\n",
    "for i in range(len(means)):\n",
    "    if i == 0:\n",
    "        print('The mean ATE for no epsilon is {} and its s.e. is {}'.format(means[i], se[i]))\n",
    "    else:\n",
    "        print('The non-privatised mean ATE for epsilon = {} is {} and its s.e. is {}'.format(epses[i-1], means[i], se[i]))                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean ATE for no epsilon is 15.03343853707586 and its s.e. is 0.35819117126040745\n",
      "The privatised mean ATE for epsilon = 0.2 is -3873162677.143155 and its s.e. is 3820306794.0853395\n",
      "The privatised mean ATE for epsilon = 0.4 is 445.35454784502053 and its s.e. is 199.18413333398016\n",
      "The privatised mean ATE for epsilon = 0.6 is 66.9816377206565 and its s.e. is 9.890741783750363\n",
      "The privatised mean ATE for epsilon = 0.8 is 27.875289015922935 and its s.e. is 1.4742573852736496\n",
      "The privatised mean ATE for epsilon = 0.99 is 22.78265811464902 and its s.e. is 0.9592348910508157\n"
     ]
    }
   ],
   "source": [
    "means = [np.mean(i) for i in te_hats_p['means']]\n",
    "se = [np.std(i) / np.sqrt(ne) for i in te_hats_p['means']]\n",
    "\n",
    "for i in range(len(means)):\n",
    "    if i == 0:\n",
    "        print('The mean ATE for no epsilon is {} and its s.e. is {}'.format(means[i], se[i]))\n",
    "    else:\n",
    "        print('The privatised mean ATE for epsilon = {} is {} and its s.e. is {}'.format(epses[i], means[i], se[i]))                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of signs being flipped for non-privatised ATE for epsilon = 0.2 is 0.528\n",
      "The probability of signs being flipped for non-privatised ATE for epsilon = 0.4 is 0.34\n",
      "The probability of signs being flipped for non-privatised ATE for epsilon = 0.6 is 0.143\n",
      "The probability of signs being flipped for non-privatised ATE for epsilon = 0.8 is 0.056\n",
      "The probability of signs being flipped for non-privatised ATE for epsilon = 0.99 is 0.015\n"
     ]
    }
   ],
   "source": [
    "sgn_tau_hat = np.sign(te_hats['means'][0])\n",
    "    \n",
    "# compute probabilities\n",
    "probs = [sum(np.sign(i) != sgn_tau_hat) / ne for i in te_hats['means'][1:]]\n",
    "\n",
    "for i in range(1, len(epses)):\n",
    "    print('The probability of signs being flipped for non-privatised ATE for epsilon = {} is {}'.format(epses[i], probs[i-1]))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of signs being flipped for privatised ATE for epsilon = 0.2 is 0.52\n",
      "The probability of signs being flipped for privatised ATE for epsilon = 0.4 is 0.321\n",
      "The probability of signs being flipped for privatised ATE for epsilon = 0.6 is 0.13\n",
      "The probability of signs being flipped for privatised ATE for epsilon = 0.8 is 0.046\n",
      "The probability of signs being flipped for privatised ATE for epsilon = 0.99 is 0.011\n"
     ]
    }
   ],
   "source": [
    "sgn_tau_hat = np.sign(te_hats_p['means'][0])\n",
    "    \n",
    "# compute probabilities\n",
    "probs = [sum((sgn_tau_hat != np.sign(te_hats['means'][1:][i])).astype('int') + \n",
    "            (sgn_tau_hat != np.sign(te_hats_p['means'][1:][i])).astype('int') == 2) / ne\n",
    "         for i in range(len(te_hats['means'][1:]))]\n",
    "\n",
    "for i in range(1, len(epses)):\n",
    "    print('The probability of signs being flipped for privatised ATE for epsilon = {} is {}'.format(epses[i], probs[i-1]))     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
